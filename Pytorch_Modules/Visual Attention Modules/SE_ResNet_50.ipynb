{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k7N4APtxyZLd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "import torchvision as tv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0SQvlqb869Sx"
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1, ratio=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels//4,\n",
    "                               kernel_size=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(num_channels//4, num_channels//4,\n",
    "                               kernel_size=3, padding=1,stride=strides)\n",
    "        self.conv3 = nn.Conv2d(num_channels//4, num_channels,\n",
    "                               kernel_size=1, padding=0)\n",
    "        \n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "            \n",
    "        self.bn1 = nn.BatchNorm2d(num_channels//4)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels//4)\n",
    "        self.bn3 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "        # SE layers\n",
    "        self.fc1 = nn.Conv2d(num_channels, num_channels//ratio, kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(num_channels//ratio, num_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        \n",
    "        # Squeeze\n",
    "        z = F.avg_pool2d(Y, Y.size(2))\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.sigmoid(self.fc2(z))\n",
    "\n",
    "        # Excitation\n",
    "        Y = Y * z \n",
    "        \n",
    "        if self.conv4:\n",
    "            X = self.conv4(X)\n",
    "            \n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1UOA_OEcFq9d"
   },
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=1, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x7vIM9dY69Zz"
   },
   "outputs": [],
   "source": [
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        elif i == 0 and first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9YmVRD1_69cY"
   },
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*resnet_block(64, 256, 3, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(256, 512, 4))\n",
    "b4 = nn.Sequential(*resnet_block(512, 1024, 6))\n",
    "b5 = nn.Sequential(*resnet_block(1024, 2048, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OAUoGUrLHUDh"
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(), nn.Dropout(0.25), nn.Linear(2048, 10))\n",
    "\n",
    "if device == 'cuda':\n",
    "\tnet = torch.nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzf_6-48H0h6",
    "outputId": "678b2fe9-c86a-4ab7-b567-4c95bd277119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 32, 32])\n",
      "Sequential output shape:\t torch.Size([1, 256, 32, 32])\n",
      "Sequential output shape:\t torch.Size([1, 512, 16, 16])\n",
      "Sequential output shape:\t torch.Size([1, 1024, 8, 8])\n",
      "Sequential output shape:\t torch.Size([1, 2048, 4, 4])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 2048, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 2048])\n",
      "Dropout output shape:\t torch.Size([1, 2048])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 3, 32, 32))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XAT7w_MlUbbK"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None): \n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  \n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OaicZn8ZPJHn"
   },
   "outputs": [],
   "source": [
    "def train_net(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            torch.nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(params=net.parameters(),\n",
    "                          lr=lr,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[65, 130, 195], gamma=0.1)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    num_batches = len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "        scheduler.step()\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(f'------- ------- Epoch{epoch+1}/{num_epochs} ------- -------')\n",
    "        print(f'loss {train_l:.5f}, train acc {train_acc:.5f}, '\n",
    "          f'test acc {test_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_aug = tv.transforms.RandomResizedCrop(\n",
    "    (28, 28), scale=(0.1, 1), ratio=(0.5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YaEWzquxpRkT"
   },
   "outputs": [],
   "source": [
    "train_augs = tv.transforms.Compose([\n",
    "    tv.transforms.RandomCrop(32, padding=4),\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.ToTensor(), shape_aug,\n",
    "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_augs = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(), \n",
    "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qGpyvAuKpOls"
   },
   "outputs": [],
   "source": [
    "def load_cifar10(is_train, augs, batch_size):\n",
    "    dataset = tv.datasets.CIFAR10(root=\"../data\", train=is_train,\n",
    "                                           transform=augs, download=True)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                    shuffle=is_train, num_workers=4)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RcP3YUwFVBbn"
   },
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.01, 250, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ECkYC1zDVVkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_iter = load_cifar10(True, train_augs, batch_size)\n",
    "test_iter = load_cifar10(False, test_augs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "------- ------- Epoch1/250 ------- -------\n",
      "loss 2.48973, train acc 0.23916, test acc 0.36900\n",
      "------- ------- Epoch2/250 ------- -------\n",
      "loss 1.83509, train acc 0.34312, test acc 0.40650\n",
      "------- ------- Epoch3/250 ------- -------\n",
      "loss 1.71162, train acc 0.38378, test acc 0.46400\n",
      "------- ------- Epoch4/250 ------- -------\n",
      "loss 1.59207, train acc 0.42652, test acc 0.48260\n",
      "------- ------- Epoch5/250 ------- -------\n",
      "loss 1.50219, train acc 0.45782, test acc 0.51710\n",
      "------- ------- Epoch6/250 ------- -------\n",
      "loss 1.43199, train acc 0.48622, test acc 0.56090\n",
      "------- ------- Epoch7/250 ------- -------\n",
      "loss 1.37253, train acc 0.50972, test acc 0.60940\n",
      "------- ------- Epoch8/250 ------- -------\n",
      "loss 1.33066, train acc 0.52294, test acc 0.58840\n",
      "------- ------- Epoch9/250 ------- -------\n",
      "loss 1.27732, train acc 0.54376, test acc 0.62440\n",
      "------- ------- Epoch10/250 ------- -------\n",
      "loss 1.23014, train acc 0.56862, test acc 0.64900\n",
      "------- ------- Epoch11/250 ------- -------\n",
      "loss 1.19199, train acc 0.57734, test acc 0.65070\n",
      "------- ------- Epoch12/250 ------- -------\n",
      "loss 1.14901, train acc 0.59454, test acc 0.66580\n",
      "------- ------- Epoch13/250 ------- -------\n",
      "loss 1.11633, train acc 0.60820, test acc 0.68380\n",
      "------- ------- Epoch14/250 ------- -------\n",
      "loss 1.08076, train acc 0.62228, test acc 0.69090\n",
      "------- ------- Epoch15/250 ------- -------\n",
      "loss 1.04886, train acc 0.63152, test acc 0.70170\n",
      "------- ------- Epoch16/250 ------- -------\n",
      "loss 1.03984, train acc 0.63326, test acc 0.73830\n",
      "------- ------- Epoch17/250 ------- -------\n",
      "loss 1.00335, train acc 0.64936, test acc 0.73440\n",
      "------- ------- Epoch18/250 ------- -------\n",
      "loss 0.99079, train acc 0.65248, test acc 0.73070\n",
      "------- ------- Epoch19/250 ------- -------\n",
      "loss 0.96805, train acc 0.66004, test acc 0.78650\n",
      "------- ------- Epoch20/250 ------- -------\n",
      "loss 0.95593, train acc 0.66720, test acc 0.77550\n",
      "------- ------- Epoch21/250 ------- -------\n",
      "loss 0.93454, train acc 0.67198, test acc 0.74420\n",
      "------- ------- Epoch22/250 ------- -------\n",
      "loss 0.91112, train acc 0.68486, test acc 0.77530\n",
      "------- ------- Epoch23/250 ------- -------\n",
      "loss 0.90277, train acc 0.68474, test acc 0.76920\n",
      "------- ------- Epoch24/250 ------- -------\n",
      "loss 0.89166, train acc 0.68750, test acc 0.79810\n",
      "------- ------- Epoch25/250 ------- -------\n",
      "loss 0.86288, train acc 0.69780, test acc 0.77980\n",
      "------- ------- Epoch26/250 ------- -------\n",
      "loss 0.86298, train acc 0.69780, test acc 0.73130\n",
      "------- ------- Epoch27/250 ------- -------\n",
      "loss 0.85396, train acc 0.70414, test acc 0.78650\n",
      "------- ------- Epoch28/250 ------- -------\n",
      "loss 0.84132, train acc 0.70916, test acc 0.82460\n",
      "------- ------- Epoch29/250 ------- -------\n",
      "loss 0.83613, train acc 0.70620, test acc 0.79640\n",
      "------- ------- Epoch30/250 ------- -------\n",
      "loss 0.82505, train acc 0.71146, test acc 0.78960\n",
      "------- ------- Epoch31/250 ------- -------\n",
      "loss 0.81409, train acc 0.71650, test acc 0.79480\n",
      "------- ------- Epoch32/250 ------- -------\n",
      "loss 0.81313, train acc 0.71500, test acc 0.82340\n",
      "------- ------- Epoch33/250 ------- -------\n",
      "loss 0.80772, train acc 0.71760, test acc 0.82910\n",
      "------- ------- Epoch34/250 ------- -------\n",
      "loss 0.79311, train acc 0.72690, test acc 0.75600\n",
      "------- ------- Epoch35/250 ------- -------\n",
      "loss 0.79232, train acc 0.72404, test acc 0.82580\n",
      "------- ------- Epoch36/250 ------- -------\n",
      "loss 0.77951, train acc 0.73104, test acc 0.82070\n",
      "------- ------- Epoch37/250 ------- -------\n",
      "loss 0.76944, train acc 0.73536, test acc 0.79210\n",
      "------- ------- Epoch38/250 ------- -------\n",
      "loss 0.77362, train acc 0.72996, test acc 0.79930\n",
      "------- ------- Epoch39/250 ------- -------\n",
      "loss 0.76453, train acc 0.73274, test acc 0.81700\n",
      "------- ------- Epoch40/250 ------- -------\n",
      "loss 0.76815, train acc 0.73358, test acc 0.84120\n",
      "------- ------- Epoch41/250 ------- -------\n",
      "loss 0.75462, train acc 0.73992, test acc 0.84360\n",
      "------- ------- Epoch42/250 ------- -------\n",
      "loss 0.75294, train acc 0.73930, test acc 0.84510\n",
      "------- ------- Epoch43/250 ------- -------\n",
      "loss 0.74448, train acc 0.74366, test acc 0.84300\n",
      "------- ------- Epoch44/250 ------- -------\n",
      "loss 0.74372, train acc 0.74234, test acc 0.84470\n",
      "------- ------- Epoch45/250 ------- -------\n",
      "loss 0.73984, train acc 0.74372, test acc 0.81090\n",
      "------- ------- Epoch46/250 ------- -------\n",
      "loss 0.74165, train acc 0.74430, test acc 0.82560\n",
      "------- ------- Epoch47/250 ------- -------\n",
      "loss 0.73560, train acc 0.74520, test acc 0.84920\n",
      "------- ------- Epoch48/250 ------- -------\n",
      "loss 0.72261, train acc 0.74966, test acc 0.84920\n",
      "------- ------- Epoch49/250 ------- -------\n",
      "loss 0.71821, train acc 0.75060, test acc 0.84460\n",
      "------- ------- Epoch50/250 ------- -------\n",
      "loss 0.72247, train acc 0.75052, test acc 0.84050\n",
      "------- ------- Epoch51/250 ------- -------\n",
      "loss 0.72414, train acc 0.74898, test acc 0.82860\n",
      "------- ------- Epoch52/250 ------- -------\n",
      "loss 0.71520, train acc 0.75410, test acc 0.83950\n",
      "------- ------- Epoch53/250 ------- -------\n",
      "loss 0.71171, train acc 0.75652, test acc 0.86110\n",
      "------- ------- Epoch54/250 ------- -------\n",
      "loss 0.71276, train acc 0.75556, test acc 0.86720\n",
      "------- ------- Epoch55/250 ------- -------\n",
      "loss 0.71506, train acc 0.75296, test acc 0.85380\n",
      "------- ------- Epoch56/250 ------- -------\n",
      "loss 0.70871, train acc 0.75356, test acc 0.84050\n",
      "------- ------- Epoch57/250 ------- -------\n",
      "loss 0.70369, train acc 0.75624, test acc 0.85060\n",
      "------- ------- Epoch58/250 ------- -------\n",
      "loss 0.70249, train acc 0.75922, test acc 0.85680\n",
      "------- ------- Epoch59/250 ------- -------\n",
      "loss 0.70301, train acc 0.75910, test acc 0.86680\n",
      "------- ------- Epoch60/250 ------- -------\n",
      "loss 0.70364, train acc 0.75714, test acc 0.85070\n",
      "------- ------- Epoch61/250 ------- -------\n",
      "loss 0.69858, train acc 0.75862, test acc 0.85080\n",
      "------- ------- Epoch62/250 ------- -------\n",
      "loss 0.70347, train acc 0.75632, test acc 0.83160\n",
      "------- ------- Epoch63/250 ------- -------\n",
      "loss 0.69633, train acc 0.76226, test acc 0.85170\n",
      "------- ------- Epoch64/250 ------- -------\n",
      "loss 0.69531, train acc 0.76122, test acc 0.83790\n",
      "------- ------- Epoch65/250 ------- -------\n",
      "loss 0.68539, train acc 0.76504, test acc 0.86380\n",
      "------- ------- Epoch66/250 ------- -------\n",
      "loss 0.56661, train acc 0.80762, test acc 0.91630\n",
      "------- ------- Epoch67/250 ------- -------\n",
      "loss 0.51386, train acc 0.82532, test acc 0.92200\n",
      "------- ------- Epoch68/250 ------- -------\n",
      "loss 0.50066, train acc 0.82894, test acc 0.92330\n",
      "------- ------- Epoch69/250 ------- -------\n",
      "loss 0.48325, train acc 0.83356, test acc 0.92400\n",
      "------- ------- Epoch70/250 ------- -------\n",
      "loss 0.47450, train acc 0.83834, test acc 0.92080\n",
      "------- ------- Epoch71/250 ------- -------\n",
      "loss 0.47439, train acc 0.83832, test acc 0.92410\n",
      "------- ------- Epoch72/250 ------- -------\n",
      "loss 0.46961, train acc 0.83978, test acc 0.92700\n",
      "------- ------- Epoch73/250 ------- -------\n",
      "loss 0.46426, train acc 0.84250, test acc 0.92680\n",
      "------- ------- Epoch74/250 ------- -------\n",
      "loss 0.45167, train acc 0.84506, test acc 0.92620\n",
      "------- ------- Epoch75/250 ------- -------\n",
      "loss 0.45733, train acc 0.84362, test acc 0.92550\n",
      "------- ------- Epoch76/250 ------- -------\n",
      "loss 0.45138, train acc 0.84524, test acc 0.92730\n",
      "------- ------- Epoch77/250 ------- -------\n",
      "loss 0.44543, train acc 0.84934, test acc 0.92850\n",
      "------- ------- Epoch78/250 ------- -------\n",
      "loss 0.43794, train acc 0.84996, test acc 0.92830\n",
      "------- ------- Epoch79/250 ------- -------\n",
      "loss 0.43823, train acc 0.84884, test acc 0.92960\n",
      "------- ------- Epoch80/250 ------- -------\n",
      "loss 0.43084, train acc 0.85336, test acc 0.92870\n",
      "------- ------- Epoch81/250 ------- -------\n",
      "loss 0.43244, train acc 0.85250, test acc 0.92950\n",
      "------- ------- Epoch82/250 ------- -------\n",
      "loss 0.43029, train acc 0.85344, test acc 0.93150\n",
      "------- ------- Epoch83/250 ------- -------\n",
      "loss 0.42519, train acc 0.85470, test acc 0.92930\n",
      "------- ------- Epoch84/250 ------- -------\n",
      "loss 0.42049, train acc 0.85548, test acc 0.92610\n",
      "------- ------- Epoch85/250 ------- -------\n",
      "loss 0.41970, train acc 0.85650, test acc 0.93030\n",
      "------- ------- Epoch86/250 ------- -------\n",
      "loss 0.41759, train acc 0.85724, test acc 0.92940\n",
      "------- ------- Epoch87/250 ------- -------\n",
      "loss 0.41520, train acc 0.85812, test acc 0.93100\n",
      "------- ------- Epoch88/250 ------- -------\n",
      "loss 0.42116, train acc 0.85554, test acc 0.93120\n",
      "------- ------- Epoch89/250 ------- -------\n",
      "loss 0.40777, train acc 0.86160, test acc 0.93210\n",
      "------- ------- Epoch90/250 ------- -------\n",
      "loss 0.40677, train acc 0.86114, test acc 0.93540\n",
      "------- ------- Epoch91/250 ------- -------\n",
      "loss 0.40581, train acc 0.86160, test acc 0.93090\n",
      "------- ------- Epoch92/250 ------- -------\n",
      "loss 0.40461, train acc 0.86108, test acc 0.93440\n",
      "------- ------- Epoch93/250 ------- -------\n",
      "loss 0.40558, train acc 0.86122, test acc 0.93590\n",
      "------- ------- Epoch94/250 ------- -------\n",
      "loss 0.39540, train acc 0.86474, test acc 0.93170\n",
      "------- ------- Epoch95/250 ------- -------\n",
      "loss 0.39739, train acc 0.86602, test acc 0.93490\n",
      "------- ------- Epoch96/250 ------- -------\n",
      "loss 0.39817, train acc 0.86434, test acc 0.93140\n",
      "------- ------- Epoch97/250 ------- -------\n",
      "loss 0.39709, train acc 0.86338, test acc 0.93600\n",
      "------- ------- Epoch98/250 ------- -------\n",
      "loss 0.40014, train acc 0.86466, test acc 0.93020\n",
      "------- ------- Epoch99/250 ------- -------\n",
      "loss 0.39053, train acc 0.86642, test acc 0.93050\n",
      "------- ------- Epoch100/250 ------- -------\n",
      "loss 0.39092, train acc 0.86828, test acc 0.93170\n",
      "------- ------- Epoch101/250 ------- -------\n",
      "loss 0.38684, train acc 0.86880, test acc 0.93290\n",
      "------- ------- Epoch102/250 ------- -------\n",
      "loss 0.38676, train acc 0.86780, test acc 0.93770\n",
      "------- ------- Epoch103/250 ------- -------\n",
      "loss 0.38557, train acc 0.86898, test acc 0.93120\n",
      "------- ------- Epoch104/250 ------- -------\n",
      "loss 0.38688, train acc 0.86864, test acc 0.93160\n",
      "------- ------- Epoch105/250 ------- -------\n",
      "loss 0.38166, train acc 0.86960, test acc 0.93500\n",
      "------- ------- Epoch106/250 ------- -------\n",
      "loss 0.38941, train acc 0.86770, test acc 0.93190\n",
      "------- ------- Epoch107/250 ------- -------\n",
      "loss 0.38391, train acc 0.87028, test acc 0.93100\n",
      "------- ------- Epoch108/250 ------- -------\n",
      "loss 0.38148, train acc 0.86958, test acc 0.93290\n",
      "------- ------- Epoch109/250 ------- -------\n",
      "loss 0.38391, train acc 0.86826, test acc 0.93670\n",
      "------- ------- Epoch110/250 ------- -------\n",
      "loss 0.38358, train acc 0.86786, test acc 0.93540\n",
      "------- ------- Epoch111/250 ------- -------\n",
      "loss 0.38024, train acc 0.87048, test acc 0.93580\n",
      "------- ------- Epoch112/250 ------- -------\n",
      "loss 0.38114, train acc 0.86960, test acc 0.93360\n",
      "------- ------- Epoch113/250 ------- -------\n",
      "loss 0.38338, train acc 0.86874, test acc 0.93190\n",
      "------- ------- Epoch114/250 ------- -------\n",
      "loss 0.37694, train acc 0.87166, test acc 0.93720\n",
      "------- ------- Epoch115/250 ------- -------\n",
      "loss 0.37066, train acc 0.87432, test acc 0.93420\n",
      "------- ------- Epoch116/250 ------- -------\n",
      "loss 0.36911, train acc 0.87330, test acc 0.93670\n",
      "------- ------- Epoch117/250 ------- -------\n",
      "loss 0.37196, train acc 0.87304, test acc 0.92700\n",
      "------- ------- Epoch118/250 ------- -------\n",
      "loss 0.37225, train acc 0.87220, test acc 0.93250\n",
      "------- ------- Epoch119/250 ------- -------\n",
      "loss 0.36918, train acc 0.87450, test acc 0.93400\n",
      "------- ------- Epoch120/250 ------- -------\n",
      "loss 0.37192, train acc 0.87352, test acc 0.93430\n",
      "------- ------- Epoch121/250 ------- -------\n",
      "loss 0.37101, train acc 0.87298, test acc 0.93160\n",
      "------- ------- Epoch122/250 ------- -------\n",
      "loss 0.37172, train acc 0.87278, test acc 0.93670\n",
      "------- ------- Epoch123/250 ------- -------\n",
      "loss 0.37190, train acc 0.87314, test acc 0.93720\n",
      "------- ------- Epoch124/250 ------- -------\n",
      "loss 0.36511, train acc 0.87472, test acc 0.93340\n",
      "------- ------- Epoch125/250 ------- -------\n",
      "loss 0.37261, train acc 0.87246, test acc 0.93340\n",
      "------- ------- Epoch126/250 ------- -------\n",
      "loss 0.36624, train acc 0.87534, test acc 0.93590\n",
      "------- ------- Epoch127/250 ------- -------\n",
      "loss 0.36354, train acc 0.87626, test acc 0.93030\n",
      "------- ------- Epoch128/250 ------- -------\n",
      "loss 0.36640, train acc 0.87482, test acc 0.93480\n",
      "------- ------- Epoch129/250 ------- -------\n",
      "loss 0.36389, train acc 0.87434, test acc 0.93580\n",
      "------- ------- Epoch130/250 ------- -------\n",
      "loss 0.36803, train acc 0.87512, test acc 0.93680\n",
      "------- ------- Epoch131/250 ------- -------\n",
      "loss 0.33766, train acc 0.88598, test acc 0.94060\n",
      "------- ------- Epoch132/250 ------- -------\n",
      "loss 0.32404, train acc 0.89008, test acc 0.94240\n",
      "------- ------- Epoch133/250 ------- -------\n",
      "loss 0.31722, train acc 0.89314, test acc 0.94180\n",
      "------- ------- Epoch134/250 ------- -------\n",
      "loss 0.31689, train acc 0.89272, test acc 0.94410\n",
      "------- ------- Epoch135/250 ------- -------\n",
      "loss 0.31202, train acc 0.89334, test acc 0.94430\n",
      "------- ------- Epoch136/250 ------- -------\n",
      "loss 0.31548, train acc 0.89416, test acc 0.94330\n",
      "------- ------- Epoch137/250 ------- -------\n",
      "loss 0.30851, train acc 0.89612, test acc 0.94500\n",
      "------- ------- Epoch138/250 ------- -------\n",
      "loss 0.31098, train acc 0.89500, test acc 0.94500\n",
      "------- ------- Epoch139/250 ------- -------\n",
      "loss 0.30187, train acc 0.89852, test acc 0.94360\n",
      "------- ------- Epoch140/250 ------- -------\n",
      "loss 0.30313, train acc 0.89736, test acc 0.94300\n",
      "------- ------- Epoch141/250 ------- -------\n",
      "loss 0.29851, train acc 0.89974, test acc 0.94440\n",
      "------- ------- Epoch142/250 ------- -------\n",
      "loss 0.30280, train acc 0.89894, test acc 0.94330\n",
      "------- ------- Epoch143/250 ------- -------\n",
      "loss 0.30190, train acc 0.89820, test acc 0.94230\n",
      "------- ------- Epoch144/250 ------- -------\n",
      "loss 0.29452, train acc 0.90062, test acc 0.94640\n",
      "------- ------- Epoch145/250 ------- -------\n",
      "loss 0.29431, train acc 0.90034, test acc 0.94650\n",
      "------- ------- Epoch146/250 ------- -------\n",
      "loss 0.29140, train acc 0.90122, test acc 0.94520\n",
      "------- ------- Epoch147/250 ------- -------\n",
      "loss 0.29676, train acc 0.90054, test acc 0.94570\n",
      "------- ------- Epoch148/250 ------- -------\n",
      "loss 0.29671, train acc 0.90066, test acc 0.94430\n",
      "------- ------- Epoch149/250 ------- -------\n",
      "loss 0.29831, train acc 0.89862, test acc 0.94510\n",
      "------- ------- Epoch150/250 ------- -------\n",
      "loss 0.29810, train acc 0.89798, test acc 0.94460\n",
      "------- ------- Epoch151/250 ------- -------\n",
      "loss 0.29320, train acc 0.90128, test acc 0.94460\n",
      "------- ------- Epoch152/250 ------- -------\n",
      "loss 0.29080, train acc 0.90160, test acc 0.94690\n",
      "------- ------- Epoch153/250 ------- -------\n",
      "loss 0.28648, train acc 0.90356, test acc 0.94330\n",
      "------- ------- Epoch154/250 ------- -------\n",
      "loss 0.28773, train acc 0.90330, test acc 0.94440\n",
      "------- ------- Epoch155/250 ------- -------\n",
      "loss 0.28896, train acc 0.90378, test acc 0.94630\n",
      "------- ------- Epoch156/250 ------- -------\n",
      "loss 0.28990, train acc 0.90274, test acc 0.94600\n",
      "------- ------- Epoch157/250 ------- -------\n",
      "loss 0.28834, train acc 0.90298, test acc 0.94620\n",
      "------- ------- Epoch158/250 ------- -------\n",
      "loss 0.28999, train acc 0.90302, test acc 0.94650\n",
      "------- ------- Epoch159/250 ------- -------\n",
      "loss 0.28632, train acc 0.90438, test acc 0.94600\n",
      "------- ------- Epoch160/250 ------- -------\n",
      "loss 0.28425, train acc 0.90428, test acc 0.94620\n",
      "------- ------- Epoch161/250 ------- -------\n",
      "loss 0.28437, train acc 0.90378, test acc 0.94510\n",
      "------- ------- Epoch162/250 ------- -------\n",
      "loss 0.28442, train acc 0.90452, test acc 0.94520\n",
      "------- ------- Epoch163/250 ------- -------\n",
      "loss 0.28994, train acc 0.90210, test acc 0.94650\n",
      "------- ------- Epoch164/250 ------- -------\n",
      "loss 0.28463, train acc 0.90480, test acc 0.94560\n",
      "------- ------- Epoch165/250 ------- -------\n",
      "loss 0.28299, train acc 0.90448, test acc 0.94550\n",
      "------- ------- Epoch166/250 ------- -------\n",
      "loss 0.28073, train acc 0.90490, test acc 0.94580\n",
      "------- ------- Epoch167/250 ------- -------\n",
      "loss 0.28862, train acc 0.90250, test acc 0.94550\n",
      "------- ------- Epoch168/250 ------- -------\n",
      "loss 0.28394, train acc 0.90418, test acc 0.94430\n",
      "------- ------- Epoch169/250 ------- -------\n",
      "loss 0.28482, train acc 0.90420, test acc 0.94530\n",
      "------- ------- Epoch170/250 ------- -------\n",
      "loss 0.28011, train acc 0.90480, test acc 0.94620\n",
      "------- ------- Epoch171/250 ------- -------\n",
      "loss 0.28191, train acc 0.90444, test acc 0.94410\n",
      "------- ------- Epoch172/250 ------- -------\n",
      "loss 0.28197, train acc 0.90568, test acc 0.94500\n",
      "------- ------- Epoch173/250 ------- -------\n",
      "loss 0.28192, train acc 0.90472, test acc 0.94640\n",
      "------- ------- Epoch174/250 ------- -------\n",
      "loss 0.28402, train acc 0.90438, test acc 0.94550\n",
      "------- ------- Epoch175/250 ------- -------\n",
      "loss 0.28048, train acc 0.90628, test acc 0.94640\n",
      "------- ------- Epoch176/250 ------- -------\n",
      "loss 0.27821, train acc 0.90594, test acc 0.94510\n",
      "------- ------- Epoch177/250 ------- -------\n",
      "loss 0.27551, train acc 0.90730, test acc 0.94590\n",
      "------- ------- Epoch178/250 ------- -------\n",
      "loss 0.28378, train acc 0.90354, test acc 0.94680\n",
      "------- ------- Epoch179/250 ------- -------\n",
      "loss 0.27696, train acc 0.90686, test acc 0.94690\n",
      "------- ------- Epoch180/250 ------- -------\n",
      "loss 0.28417, train acc 0.90384, test acc 0.94720\n",
      "------- ------- Epoch181/250 ------- -------\n",
      "loss 0.28031, train acc 0.90542, test acc 0.94550\n",
      "------- ------- Epoch182/250 ------- -------\n",
      "loss 0.27222, train acc 0.90844, test acc 0.94560\n",
      "------- ------- Epoch183/250 ------- -------\n",
      "loss 0.27847, train acc 0.90586, test acc 0.94630\n",
      "------- ------- Epoch184/250 ------- -------\n",
      "loss 0.27891, train acc 0.90610, test acc 0.94360\n",
      "------- ------- Epoch185/250 ------- -------\n",
      "loss 0.27470, train acc 0.90708, test acc 0.94620\n",
      "------- ------- Epoch186/250 ------- -------\n",
      "loss 0.27492, train acc 0.90632, test acc 0.94610\n",
      "------- ------- Epoch187/250 ------- -------\n",
      "loss 0.27416, train acc 0.90662, test acc 0.94620\n",
      "------- ------- Epoch188/250 ------- -------\n",
      "loss 0.27184, train acc 0.90934, test acc 0.94450\n",
      "------- ------- Epoch189/250 ------- -------\n",
      "loss 0.27533, train acc 0.90642, test acc 0.94610\n",
      "------- ------- Epoch190/250 ------- -------\n",
      "loss 0.27117, train acc 0.90934, test acc 0.94450\n",
      "------- ------- Epoch191/250 ------- -------\n",
      "loss 0.26894, train acc 0.90964, test acc 0.94560\n",
      "------- ------- Epoch192/250 ------- -------\n",
      "loss 0.27792, train acc 0.90558, test acc 0.94550\n",
      "------- ------- Epoch193/250 ------- -------\n",
      "loss 0.27504, train acc 0.90656, test acc 0.94710\n",
      "------- ------- Epoch194/250 ------- -------\n",
      "loss 0.27445, train acc 0.90712, test acc 0.94680\n",
      "------- ------- Epoch195/250 ------- -------\n",
      "loss 0.26891, train acc 0.91068, test acc 0.94690\n",
      "------- ------- Epoch196/250 ------- -------\n",
      "loss 0.27146, train acc 0.90784, test acc 0.94640\n",
      "------- ------- Epoch197/250 ------- -------\n",
      "loss 0.26860, train acc 0.90922, test acc 0.94630\n",
      "------- ------- Epoch198/250 ------- -------\n",
      "loss 0.26863, train acc 0.90910, test acc 0.94730\n",
      "------- ------- Epoch199/250 ------- -------\n",
      "loss 0.26826, train acc 0.90966, test acc 0.94770\n",
      "------- ------- Epoch200/250 ------- -------\n",
      "loss 0.26707, train acc 0.90982, test acc 0.94510\n",
      "------- ------- Epoch201/250 ------- -------\n",
      "loss 0.26423, train acc 0.91090, test acc 0.94760\n",
      "------- ------- Epoch202/250 ------- -------\n",
      "loss 0.27097, train acc 0.90818, test acc 0.94620\n",
      "------- ------- Epoch203/250 ------- -------\n",
      "loss 0.27203, train acc 0.90878, test acc 0.94620\n",
      "------- ------- Epoch204/250 ------- -------\n",
      "loss 0.26546, train acc 0.91092, test acc 0.94760\n",
      "------- ------- Epoch205/250 ------- -------\n",
      "loss 0.26907, train acc 0.90864, test acc 0.94610\n",
      "------- ------- Epoch206/250 ------- -------\n",
      "loss 0.26576, train acc 0.90960, test acc 0.94650\n",
      "------- ------- Epoch207/250 ------- -------\n",
      "loss 0.26752, train acc 0.91032, test acc 0.94830\n",
      "------- ------- Epoch208/250 ------- -------\n",
      "loss 0.26229, train acc 0.91108, test acc 0.94670\n",
      "------- ------- Epoch209/250 ------- -------\n",
      "loss 0.26450, train acc 0.91064, test acc 0.94670\n",
      "------- ------- Epoch210/250 ------- -------\n",
      "loss 0.26153, train acc 0.91124, test acc 0.94630\n",
      "------- ------- Epoch211/250 ------- -------\n",
      "loss 0.26784, train acc 0.91098, test acc 0.94780\n",
      "------- ------- Epoch212/250 ------- -------\n",
      "loss 0.26471, train acc 0.91142, test acc 0.94770\n",
      "------- ------- Epoch213/250 ------- -------\n",
      "loss 0.26247, train acc 0.91210, test acc 0.94770\n",
      "------- ------- Epoch214/250 ------- -------\n",
      "loss 0.26687, train acc 0.90938, test acc 0.94770\n",
      "------- ------- Epoch215/250 ------- -------\n",
      "loss 0.26749, train acc 0.91054, test acc 0.94720\n",
      "------- ------- Epoch216/250 ------- -------\n",
      "loss 0.26512, train acc 0.91130, test acc 0.94790\n",
      "------- ------- Epoch217/250 ------- -------\n",
      "loss 0.26962, train acc 0.90872, test acc 0.94690\n",
      "------- ------- Epoch218/250 ------- -------\n",
      "loss 0.26182, train acc 0.91212, test acc 0.94650\n",
      "------- ------- Epoch219/250 ------- -------\n",
      "loss 0.26169, train acc 0.91262, test acc 0.94810\n",
      "------- ------- Epoch220/250 ------- -------\n",
      "loss 0.26902, train acc 0.90866, test acc 0.94770\n",
      "------- ------- Epoch221/250 ------- -------\n",
      "loss 0.26391, train acc 0.91190, test acc 0.94670\n",
      "------- ------- Epoch222/250 ------- -------\n",
      "loss 0.26972, train acc 0.90966, test acc 0.94740\n",
      "------- ------- Epoch223/250 ------- -------\n",
      "loss 0.26938, train acc 0.90966, test acc 0.94730\n",
      "------- ------- Epoch224/250 ------- -------\n",
      "loss 0.26786, train acc 0.90906, test acc 0.94720\n",
      "------- ------- Epoch225/250 ------- -------\n",
      "loss 0.26239, train acc 0.91186, test acc 0.94710\n",
      "------- ------- Epoch226/250 ------- -------\n",
      "loss 0.26052, train acc 0.91252, test acc 0.94700\n",
      "------- ------- Epoch227/250 ------- -------\n",
      "loss 0.26510, train acc 0.91102, test acc 0.94650\n",
      "------- ------- Epoch228/250 ------- -------\n",
      "loss 0.26528, train acc 0.91072, test acc 0.94680\n",
      "------- ------- Epoch229/250 ------- -------\n",
      "loss 0.26336, train acc 0.91256, test acc 0.94670\n",
      "------- ------- Epoch230/250 ------- -------\n",
      "loss 0.26187, train acc 0.91248, test acc 0.94630\n",
      "------- ------- Epoch231/250 ------- -------\n",
      "loss 0.26314, train acc 0.91200, test acc 0.94750\n",
      "------- ------- Epoch232/250 ------- -------\n",
      "loss 0.26274, train acc 0.91152, test acc 0.94760\n",
      "------- ------- Epoch233/250 ------- -------\n",
      "loss 0.26082, train acc 0.91164, test acc 0.94780\n",
      "------- ------- Epoch234/250 ------- -------\n",
      "loss 0.26703, train acc 0.91026, test acc 0.94750\n",
      "------- ------- Epoch235/250 ------- -------\n",
      "loss 0.26433, train acc 0.91134, test acc 0.94720\n",
      "------- ------- Epoch236/250 ------- -------\n",
      "loss 0.26267, train acc 0.91178, test acc 0.94780\n",
      "------- ------- Epoch237/250 ------- -------\n",
      "loss 0.26252, train acc 0.91064, test acc 0.94710\n",
      "------- ------- Epoch238/250 ------- -------\n",
      "loss 0.26539, train acc 0.90962, test acc 0.94770\n",
      "------- ------- Epoch239/250 ------- -------\n",
      "loss 0.26923, train acc 0.90914, test acc 0.94800\n",
      "------- ------- Epoch240/250 ------- -------\n",
      "loss 0.26395, train acc 0.91080, test acc 0.94630\n",
      "------- ------- Epoch241/250 ------- -------\n",
      "loss 0.26619, train acc 0.91166, test acc 0.94760\n",
      "------- ------- Epoch242/250 ------- -------\n",
      "loss 0.26376, train acc 0.91050, test acc 0.94730\n",
      "------- ------- Epoch243/250 ------- -------\n",
      "loss 0.26886, train acc 0.90912, test acc 0.94760\n",
      "------- ------- Epoch244/250 ------- -------\n",
      "loss 0.26592, train acc 0.90958, test acc 0.94700\n",
      "------- ------- Epoch245/250 ------- -------\n",
      "loss 0.26472, train acc 0.90998, test acc 0.94800\n",
      "------- ------- Epoch246/250 ------- -------\n",
      "loss 0.26362, train acc 0.91144, test acc 0.94750\n",
      "------- ------- Epoch247/250 ------- -------\n",
      "loss 0.26157, train acc 0.91316, test acc 0.94760\n",
      "------- ------- Epoch248/250 ------- -------\n",
      "loss 0.26258, train acc 0.91136, test acc 0.94820\n",
      "------- ------- Epoch249/250 ------- -------\n",
      "loss 0.26506, train acc 0.91080, test acc 0.94770\n",
      "------- ------- Epoch250/250 ------- -------\n",
      "loss 0.26552, train acc 0.91080, test acc 0.94890\n"
     ]
    }
   ],
   "source": [
    "train_net(net, train_iter, test_iter, num_epochs, lr, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet_34.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

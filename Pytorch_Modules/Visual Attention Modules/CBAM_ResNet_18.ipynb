{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "import torchvision as tv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, num_channels, ratio=16):\n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    " \n",
    "        self.shared_MLP = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_channels // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels // ratio, num_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, X):\n",
    "        avgout = self.shared_MLP(self.avg_pool(X))\n",
    "        maxout = self.shared_MLP(self.max_pool(X))\n",
    "        return self.sigmoid(avgout + maxout)\n",
    " \n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, X):\n",
    "        avgout = torch.mean(X, dim=1, keepdim=True)\n",
    "        maxout, _ = torch.max(X, dim=1, keepdim=True)\n",
    "        Y = torch.cat([avgout, maxout], dim=1)\n",
    "        Y = self.sigmoid(self.conv2d(Y))\n",
    "        return Y\n",
    " \n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttentionModule(num_channels)\n",
    "        self.spatial_attention = SpatialAttentionModule()\n",
    " \n",
    "    def forward(self, X):\n",
    "        Y = self.channel_attention(X) * X\n",
    "        Y = self.spatial_attention(Y) * Y\n",
    "        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0SQvlqb869Sx"
   },
   "outputs": [],
   "source": [
    "class CBAM_Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "            \n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "        self.cbam = CBAM(num_channels)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        \n",
    "        Y = self.cbam(Y)\n",
    "        \n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1UOA_OEcFq9d"
   },
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=1, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x7vIM9dY69Zz"
   },
   "outputs": [],
   "source": [
    "def CBAM_resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(CBAM_Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(CBAM_Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9YmVRD1_69cY"
   },
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*CBAM_resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*CBAM_resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*CBAM_resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*CBAM_resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OAUoGUrLHUDh"
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(), nn.Dropout(0.25), nn.Linear(512, 10))\n",
    "\n",
    "if device == 'cuda':\n",
    "\tnet = torch.nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xzf_6-48H0h6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 32, 32])\n",
      "Sequential output shape:\t torch.Size([1, 64, 32, 32])\n",
      "Sequential output shape:\t torch.Size([1, 128, 16, 16])\n",
      "Sequential output shape:\t torch.Size([1, 256, 8, 8])\n",
      "Sequential output shape:\t torch.Size([1, 512, 4, 4])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Dropout output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 3, 32, 32))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XAT7w_MlUbbK"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None): \n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  \n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OaicZn8ZPJHn"
   },
   "outputs": [],
   "source": [
    "def train_net(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            torch.nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(params=net.parameters(),\n",
    "                          lr=lr,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[50, 90, 120], gamma=0.1)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "        scheduler.step()\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(f'------- -------- Epoch{epoch+1}/{num_epochs} ------- -------')\n",
    "        print(f'loss {train_l:.5f}, train acc {train_acc:.5f}, '\n",
    "           f'test acc {test_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PaKcZz8N3YAZ"
   },
   "outputs": [],
   "source": [
    "shape_aug = tv.transforms.RandomResizedCrop(\n",
    "    (28, 28), scale=(0.1, 1), ratio=(0.5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YaEWzquxpRkT"
   },
   "outputs": [],
   "source": [
    "train_augs = tv.transforms.Compose([\n",
    "    tv.transforms.RandomCrop(32, padding=4),\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.ToTensor(), shape_aug,\n",
    "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_augs = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qGpyvAuKpOls"
   },
   "outputs": [],
   "source": [
    "def load_cifar10(is_train, augs, batch_size):\n",
    "    dataset = tv.datasets.CIFAR10(root=\"../data\", train=is_train,\n",
    "                                           transform=augs, download=True)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                    shuffle=is_train, num_workers=2)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RcP3YUwFVBbn"
   },
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.01, 150, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_iter = load_cifar10(True, train_augs, batch_size)\n",
    "test_iter = load_cifar10(False, test_augs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "------- -------- Epoch1/150 ------- -------\n",
      "loss 1.89083, train acc 0.30774, test acc 0.40680\n",
      "------- -------- Epoch2/150 ------- -------\n",
      "loss 1.62597, train acc 0.40736, test acc 0.49380\n",
      "------- -------- Epoch3/150 ------- -------\n",
      "loss 1.49939, train acc 0.45564, test acc 0.55130\n",
      "------- -------- Epoch4/150 ------- -------\n",
      "loss 1.41044, train acc 0.49280, test acc 0.56690\n",
      "------- -------- Epoch5/150 ------- -------\n",
      "loss 1.33137, train acc 0.52464, test acc 0.52760\n",
      "------- -------- Epoch6/150 ------- -------\n",
      "loss 1.27236, train acc 0.54712, test acc 0.61920\n",
      "------- -------- Epoch7/150 ------- -------\n",
      "loss 1.22122, train acc 0.56724, test acc 0.68010\n",
      "------- -------- Epoch8/150 ------- -------\n",
      "loss 1.17176, train acc 0.58388, test acc 0.67640\n",
      "------- -------- Epoch9/150 ------- -------\n",
      "loss 1.12791, train acc 0.60124, test acc 0.69310\n",
      "------- -------- Epoch10/150 ------- -------\n",
      "loss 1.09250, train acc 0.61466, test acc 0.65730\n",
      "------- -------- Epoch11/150 ------- -------\n",
      "loss 1.06783, train acc 0.62428, test acc 0.74650\n",
      "------- -------- Epoch12/150 ------- -------\n",
      "loss 1.03573, train acc 0.63652, test acc 0.73540\n",
      "------- -------- Epoch13/150 ------- -------\n",
      "loss 1.00377, train acc 0.64724, test acc 0.75940\n",
      "------- -------- Epoch14/150 ------- -------\n",
      "loss 0.99290, train acc 0.65082, test acc 0.73460\n",
      "------- -------- Epoch15/150 ------- -------\n",
      "loss 0.97118, train acc 0.65972, test acc 0.74650\n",
      "------- -------- Epoch16/150 ------- -------\n",
      "loss 0.95141, train acc 0.66936, test acc 0.77270\n",
      "------- -------- Epoch17/150 ------- -------\n",
      "loss 0.93044, train acc 0.67350, test acc 0.75820\n",
      "------- -------- Epoch18/150 ------- -------\n",
      "loss 0.91600, train acc 0.67926, test acc 0.76480\n",
      "------- -------- Epoch19/150 ------- -------\n",
      "loss 0.89705, train acc 0.68782, test acc 0.78120\n",
      "------- -------- Epoch20/150 ------- -------\n",
      "loss 0.89075, train acc 0.69046, test acc 0.76950\n",
      "------- -------- Epoch21/150 ------- -------\n",
      "loss 0.87884, train acc 0.69320, test acc 0.79460\n",
      "------- -------- Epoch22/150 ------- -------\n",
      "loss 0.87234, train acc 0.69492, test acc 0.80210\n",
      "------- -------- Epoch23/150 ------- -------\n",
      "loss 0.85237, train acc 0.70496, test acc 0.82940\n",
      "------- -------- Epoch24/150 ------- -------\n",
      "loss 0.84456, train acc 0.70776, test acc 0.79140\n",
      "------- -------- Epoch25/150 ------- -------\n",
      "loss 0.83776, train acc 0.70652, test acc 0.76920\n",
      "------- -------- Epoch26/150 ------- -------\n",
      "loss 0.83090, train acc 0.71186, test acc 0.81040\n",
      "------- -------- Epoch27/150 ------- -------\n",
      "loss 0.82369, train acc 0.71366, test acc 0.83720\n",
      "------- -------- Epoch28/150 ------- -------\n",
      "loss 0.80977, train acc 0.71790, test acc 0.82310\n",
      "------- -------- Epoch29/150 ------- -------\n",
      "loss 0.80337, train acc 0.72062, test acc 0.81330\n",
      "------- -------- Epoch30/150 ------- -------\n",
      "loss 0.79560, train acc 0.72328, test acc 0.85190\n",
      "------- -------- Epoch31/150 ------- -------\n",
      "loss 0.77944, train acc 0.72876, test acc 0.83270\n",
      "------- -------- Epoch32/150 ------- -------\n",
      "loss 0.78286, train acc 0.73090, test acc 0.83490\n",
      "------- -------- Epoch33/150 ------- -------\n",
      "loss 0.77889, train acc 0.73134, test acc 0.84660\n",
      "------- -------- Epoch34/150 ------- -------\n",
      "loss 0.77518, train acc 0.73164, test acc 0.83170\n",
      "------- -------- Epoch35/150 ------- -------\n",
      "loss 0.76570, train acc 0.73410, test acc 0.82800\n",
      "------- -------- Epoch36/150 ------- -------\n",
      "loss 0.76159, train acc 0.73756, test acc 0.84390\n",
      "------- -------- Epoch37/150 ------- -------\n",
      "loss 0.75923, train acc 0.73708, test acc 0.84370\n",
      "------- -------- Epoch38/150 ------- -------\n",
      "loss 0.75376, train acc 0.73896, test acc 0.85800\n",
      "------- -------- Epoch39/150 ------- -------\n",
      "loss 0.74550, train acc 0.74166, test acc 0.83370\n",
      "------- -------- Epoch40/150 ------- -------\n",
      "loss 0.74497, train acc 0.74466, test acc 0.84840\n",
      "------- -------- Epoch41/150 ------- -------\n",
      "loss 0.73914, train acc 0.74562, test acc 0.85660\n",
      "------- -------- Epoch42/150 ------- -------\n",
      "loss 0.73001, train acc 0.74734, test acc 0.84810\n",
      "------- -------- Epoch43/150 ------- -------\n",
      "loss 0.73524, train acc 0.74556, test acc 0.81570\n",
      "------- -------- Epoch44/150 ------- -------\n",
      "loss 0.72480, train acc 0.75118, test acc 0.86640\n",
      "------- -------- Epoch45/150 ------- -------\n",
      "loss 0.71474, train acc 0.75262, test acc 0.86790\n",
      "------- -------- Epoch46/150 ------- -------\n",
      "loss 0.71859, train acc 0.75318, test acc 0.85320\n",
      "------- -------- Epoch47/150 ------- -------\n",
      "loss 0.71975, train acc 0.75128, test acc 0.85170\n",
      "------- -------- Epoch48/150 ------- -------\n",
      "loss 0.70129, train acc 0.75666, test acc 0.87120\n",
      "------- -------- Epoch49/150 ------- -------\n",
      "loss 0.70584, train acc 0.75604, test acc 0.85900\n",
      "------- -------- Epoch50/150 ------- -------\n",
      "loss 0.71003, train acc 0.75558, test acc 0.85790\n",
      "------- -------- Epoch51/150 ------- -------\n",
      "loss 0.58767, train acc 0.79896, test acc 0.90530\n",
      "------- -------- Epoch52/150 ------- -------\n",
      "loss 0.55395, train acc 0.81156, test acc 0.90680\n",
      "------- -------- Epoch53/150 ------- -------\n",
      "loss 0.53636, train acc 0.81716, test acc 0.91210\n",
      "------- -------- Epoch54/150 ------- -------\n",
      "loss 0.52631, train acc 0.81924, test acc 0.91050\n",
      "------- -------- Epoch55/150 ------- -------\n",
      "loss 0.51922, train acc 0.82202, test acc 0.91160\n",
      "------- -------- Epoch56/150 ------- -------\n",
      "loss 0.50886, train acc 0.82604, test acc 0.91510\n",
      "------- -------- Epoch57/150 ------- -------\n",
      "loss 0.49908, train acc 0.82822, test acc 0.91260\n",
      "------- -------- Epoch58/150 ------- -------\n",
      "loss 0.49626, train acc 0.82882, test acc 0.91430\n",
      "------- -------- Epoch59/150 ------- -------\n",
      "loss 0.49026, train acc 0.83122, test acc 0.91640\n",
      "------- -------- Epoch60/150 ------- -------\n",
      "loss 0.48871, train acc 0.83240, test acc 0.91500\n",
      "------- -------- Epoch61/150 ------- -------\n",
      "loss 0.48254, train acc 0.83326, test acc 0.91540\n",
      "------- -------- Epoch62/150 ------- -------\n",
      "loss 0.48297, train acc 0.83384, test acc 0.91520\n",
      "------- -------- Epoch63/150 ------- -------\n",
      "loss 0.47589, train acc 0.83612, test acc 0.91700\n",
      "------- -------- Epoch64/150 ------- -------\n",
      "loss 0.46509, train acc 0.84072, test acc 0.91530\n",
      "------- -------- Epoch65/150 ------- -------\n",
      "loss 0.47261, train acc 0.83882, test acc 0.91590\n",
      "------- -------- Epoch66/150 ------- -------\n",
      "loss 0.46298, train acc 0.84126, test acc 0.91860\n",
      "------- -------- Epoch67/150 ------- -------\n",
      "loss 0.47086, train acc 0.83710, test acc 0.92090\n",
      "------- -------- Epoch68/150 ------- -------\n",
      "loss 0.45912, train acc 0.84334, test acc 0.91890\n",
      "------- -------- Epoch69/150 ------- -------\n",
      "loss 0.46526, train acc 0.84184, test acc 0.92080\n",
      "------- -------- Epoch70/150 ------- -------\n",
      "loss 0.45827, train acc 0.84318, test acc 0.92180\n",
      "------- -------- Epoch71/150 ------- -------\n",
      "loss 0.46268, train acc 0.84162, test acc 0.91860\n",
      "------- -------- Epoch72/150 ------- -------\n",
      "loss 0.45015, train acc 0.84352, test acc 0.92130\n",
      "------- -------- Epoch73/150 ------- -------\n",
      "loss 0.45325, train acc 0.84536, test acc 0.92270\n",
      "------- -------- Epoch74/150 ------- -------\n",
      "loss 0.44347, train acc 0.84612, test acc 0.91880\n",
      "------- -------- Epoch75/150 ------- -------\n",
      "loss 0.45076, train acc 0.84480, test acc 0.92130\n",
      "------- -------- Epoch76/150 ------- -------\n",
      "loss 0.44549, train acc 0.84936, test acc 0.91580\n",
      "------- -------- Epoch77/150 ------- -------\n",
      "loss 0.43887, train acc 0.84966, test acc 0.91900\n",
      "------- -------- Epoch78/150 ------- -------\n",
      "loss 0.44449, train acc 0.84666, test acc 0.91920\n",
      "------- -------- Epoch79/150 ------- -------\n",
      "loss 0.44589, train acc 0.84584, test acc 0.91920\n",
      "------- -------- Epoch80/150 ------- -------\n",
      "loss 0.43603, train acc 0.85072, test acc 0.92040\n",
      "------- -------- Epoch81/150 ------- -------\n",
      "loss 0.43329, train acc 0.84994, test acc 0.91580\n",
      "------- -------- Epoch82/150 ------- -------\n",
      "loss 0.42917, train acc 0.85322, test acc 0.91920\n",
      "------- -------- Epoch83/150 ------- -------\n",
      "loss 0.43511, train acc 0.85164, test acc 0.92070\n",
      "------- -------- Epoch84/150 ------- -------\n",
      "loss 0.43714, train acc 0.84936, test acc 0.92510\n",
      "------- -------- Epoch85/150 ------- -------\n",
      "loss 0.43168, train acc 0.85166, test acc 0.92660\n",
      "------- -------- Epoch86/150 ------- -------\n",
      "loss 0.43054, train acc 0.85288, test acc 0.92450\n",
      "------- -------- Epoch87/150 ------- -------\n",
      "loss 0.42562, train acc 0.85338, test acc 0.92350\n",
      "------- -------- Epoch88/150 ------- -------\n",
      "loss 0.42410, train acc 0.85502, test acc 0.92090\n",
      "------- -------- Epoch89/150 ------- -------\n",
      "loss 0.42370, train acc 0.85272, test acc 0.92090\n",
      "------- -------- Epoch90/150 ------- -------\n",
      "loss 0.43249, train acc 0.85156, test acc 0.92400\n",
      "------- -------- Epoch91/150 ------- -------\n",
      "loss 0.40515, train acc 0.86080, test acc 0.92830\n",
      "------- -------- Epoch92/150 ------- -------\n",
      "loss 0.39840, train acc 0.86394, test acc 0.92850\n",
      "------- -------- Epoch93/150 ------- -------\n",
      "loss 0.39516, train acc 0.86380, test acc 0.92750\n",
      "------- -------- Epoch94/150 ------- -------\n",
      "loss 0.38835, train acc 0.86806, test acc 0.92780\n",
      "------- -------- Epoch95/150 ------- -------\n",
      "loss 0.38640, train acc 0.86818, test acc 0.92880\n",
      "------- -------- Epoch96/150 ------- -------\n",
      "loss 0.38378, train acc 0.86786, test acc 0.92850\n",
      "------- -------- Epoch97/150 ------- -------\n",
      "loss 0.39080, train acc 0.86550, test acc 0.92790\n",
      "------- -------- Epoch98/150 ------- -------\n",
      "loss 0.38227, train acc 0.86830, test acc 0.92970\n",
      "------- -------- Epoch99/150 ------- -------\n",
      "loss 0.38559, train acc 0.86740, test acc 0.92770\n",
      "------- -------- Epoch100/150 ------- -------\n",
      "loss 0.37854, train acc 0.87130, test acc 0.92830\n",
      "------- -------- Epoch101/150 ------- -------\n",
      "loss 0.37662, train acc 0.87266, test acc 0.93120\n",
      "------- -------- Epoch102/150 ------- -------\n",
      "loss 0.37474, train acc 0.87160, test acc 0.92950\n",
      "------- -------- Epoch103/150 ------- -------\n",
      "loss 0.37706, train acc 0.87218, test acc 0.92900\n",
      "------- -------- Epoch104/150 ------- -------\n",
      "loss 0.37947, train acc 0.87014, test acc 0.93000\n",
      "------- -------- Epoch105/150 ------- -------\n",
      "loss 0.37661, train acc 0.87032, test acc 0.92840\n",
      "------- -------- Epoch106/150 ------- -------\n",
      "loss 0.37525, train acc 0.87154, test acc 0.92780\n",
      "------- -------- Epoch107/150 ------- -------\n",
      "loss 0.37717, train acc 0.87162, test acc 0.92780\n",
      "------- -------- Epoch108/150 ------- -------\n",
      "loss 0.37413, train acc 0.87134, test acc 0.92910\n",
      "------- -------- Epoch109/150 ------- -------\n",
      "loss 0.37582, train acc 0.87150, test acc 0.93010\n",
      "------- -------- Epoch110/150 ------- -------\n",
      "loss 0.37305, train acc 0.87136, test acc 0.92950\n",
      "------- -------- Epoch111/150 ------- -------\n",
      "loss 0.37197, train acc 0.87412, test acc 0.93080\n",
      "------- -------- Epoch112/150 ------- -------\n",
      "loss 0.37343, train acc 0.87118, test acc 0.92900\n",
      "------- -------- Epoch113/150 ------- -------\n",
      "loss 0.37574, train acc 0.87172, test acc 0.92900\n",
      "------- -------- Epoch114/150 ------- -------\n",
      "loss 0.36970, train acc 0.87302, test acc 0.92930\n",
      "------- -------- Epoch115/150 ------- -------\n",
      "loss 0.36939, train acc 0.87430, test acc 0.92800\n",
      "------- -------- Epoch116/150 ------- -------\n",
      "loss 0.36433, train acc 0.87638, test acc 0.92740\n",
      "------- -------- Epoch120/150 ------- -------\n",
      "loss 0.36483, train acc 0.87588, test acc 0.92990\n",
      "------- -------- Epoch121/150 ------- -------\n",
      "loss 0.37366, train acc 0.87360, test acc 0.93150\n",
      "------- -------- Epoch122/150 ------- -------\n",
      "loss 0.37287, train acc 0.87180, test acc 0.93050\n",
      "------- -------- Epoch123/150 ------- -------\n",
      "loss 0.36651, train acc 0.87414, test acc 0.93080\n",
      "------- -------- Epoch124/150 ------- -------\n",
      "loss 0.36389, train acc 0.87612, test acc 0.93110\n",
      "------- -------- Epoch125/150 ------- -------\n",
      "loss 0.36215, train acc 0.87628, test acc 0.93010\n",
      "------- -------- Epoch126/150 ------- -------\n",
      "loss 0.36126, train acc 0.87610, test acc 0.93090\n",
      "------- -------- Epoch127/150 ------- -------\n",
      "loss 0.36530, train acc 0.87614, test acc 0.93060\n",
      "------- -------- Epoch128/150 ------- -------\n",
      "loss 0.36667, train acc 0.87526, test acc 0.92980\n",
      "------- -------- Epoch129/150 ------- -------\n",
      "loss 0.36381, train acc 0.87502, test acc 0.93140\n",
      "------- -------- Epoch130/150 ------- -------\n",
      "loss 0.36453, train acc 0.87662, test acc 0.93220\n",
      "------- -------- Epoch131/150 ------- -------\n",
      "loss 0.36419, train acc 0.87692, test acc 0.93190\n",
      "------- -------- Epoch132/150 ------- -------\n",
      "loss 0.36480, train acc 0.87482, test acc 0.93170\n",
      "------- -------- Epoch133/150 ------- -------\n",
      "loss 0.36023, train acc 0.87766, test acc 0.93120\n",
      "------- -------- Epoch134/150 ------- -------\n",
      "loss 0.36545, train acc 0.87744, test acc 0.92940\n",
      "------- -------- Epoch135/150 ------- -------\n",
      "loss 0.37042, train acc 0.87364, test acc 0.93110\n",
      "------- -------- Epoch136/150 ------- -------\n",
      "loss 0.35925, train acc 0.87740, test acc 0.92980\n",
      "------- -------- Epoch137/150 ------- -------\n",
      "loss 0.35897, train acc 0.87742, test acc 0.93150\n",
      "------- -------- Epoch138/150 ------- -------\n",
      "loss 0.35812, train acc 0.87838, test acc 0.93060\n",
      "------- -------- Epoch139/150 ------- -------\n",
      "loss 0.36981, train acc 0.87396, test acc 0.93190\n",
      "------- -------- Epoch140/150 ------- -------\n",
      "loss 0.36657, train acc 0.87460, test acc 0.92990\n",
      "------- -------- Epoch141/150 ------- -------\n",
      "loss 0.36584, train acc 0.87582, test acc 0.93080\n",
      "------- -------- Epoch142/150 ------- -------\n",
      "loss 0.36217, train acc 0.87744, test acc 0.93130\n",
      "------- -------- Epoch143/150 ------- -------\n",
      "loss 0.36470, train acc 0.87518, test acc 0.93080\n",
      "------- -------- Epoch144/150 ------- -------\n",
      "loss 0.36025, train acc 0.87600, test acc 0.93020\n",
      "------- -------- Epoch145/150 ------- -------\n",
      "loss 0.36092, train acc 0.87602, test acc 0.93180\n",
      "------- -------- Epoch146/150 ------- -------\n",
      "loss 0.36169, train acc 0.87568, test acc 0.93050\n",
      "------- -------- Epoch147/150 ------- -------\n",
      "loss 0.36632, train acc 0.87592, test acc 0.93070\n",
      "------- -------- Epoch148/150 ------- -------\n",
      "loss 0.36349, train acc 0.87702, test acc 0.93090\n",
      "------- -------- Epoch149/150 ------- -------\n",
      "loss 0.36567, train acc 0.87378, test acc 0.93120\n",
      "------- -------- Epoch150/150 ------- -------\n",
      "loss 0.36243, train acc 0.87504, test acc 0.93080\n"
     ]
    }
   ],
   "source": [
    "train_net(net, train_iter, test_iter, num_epochs, lr, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CBAM_ResNet_18.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

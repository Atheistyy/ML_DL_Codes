{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l\n",
        "!pip install matplotlib-inline"
      ],
      "metadata": {
        "id": "9ZCOTXLZQ4kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7N4APtxyZLd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        "import torchvision as tv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SQvlqb869Sx"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_channels, num_channels,\n",
        "                 use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
        "                               kernel_size=3, padding=1, stride=strides)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
        "                                   kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7vIM9dY69Zz"
      },
      "outputs": [],
      "source": [
        "def resnet_block(input_channels, num_channels, num_residuals,\n",
        "                 first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(input_channels, num_channels,\n",
        "                                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels, num_channels))\n",
        "    return blk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDgpSg1mL4Dk"
      },
      "outputs": [],
      "source": [
        "b1 = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "                   nn.BatchNorm2d(16), nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride=1, padding=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YmVRD1_69cY"
      },
      "outputs": [],
      "source": [
        "b2 = nn.Sequential(*resnet_block(16, 16, 3, first_block=True))\n",
        "b3 = nn.Sequential(*resnet_block(16, 32, 3))\n",
        "b4 = nn.Sequential(*resnet_block(32, 64, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAUoGUrLHUDh"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(b1, b2, b3, b4,\n",
        "                    nn.AdaptiveAvgPool2d((1,1)),\n",
        "                    nn.Flatten(), nn.Dropout(0.1), nn.Linear(64, 10))\n",
        "\n",
        "if device == 'cuda':\n",
        "\tnet = torch.nn.DataParallel(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzf_6-48H0h6",
        "outputId": "23684b5e-e6d4-4c0f-bf71-de6afea23797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 16, 32, 32])\n",
            "Sequential output shape:\t torch.Size([1, 16, 32, 32])\n",
            "Sequential output shape:\t torch.Size([1, 32, 16, 16])\n",
            "Sequential output shape:\t torch.Size([1, 64, 8, 8])\n",
            "AdaptiveAvgPool2d output shape:\t torch.Size([1, 64, 1, 1])\n",
            "Flatten output shape:\t torch.Size([1, 64])\n",
            "Dropout output shape:\t torch.Size([1, 64])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(size=(1, 3, 32, 32))\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAT7w_MlUbbK"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy_gpu(net, data_iter, device=None): \n",
        "    if isinstance(net, nn.Module):\n",
        "        net.eval()  \n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    metric = d2l.Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            if isinstance(X, list):\n",
        "                X = [x.to(device) for x in X]\n",
        "            else:\n",
        "                X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
        "    return metric[0] / metric[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaicZn8ZPJHn"
      },
      "outputs": [],
      "source": [
        "def train_net(net, train_iter, test_iter, num_epochs, lr, device):\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            torch.nn.init.kaiming_uniform_(m.weight)\n",
        "    net.apply(init_weights)\n",
        "    print('training on', device)\n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.SGD(params=net.parameters(),\n",
        "                          lr=lr,\n",
        "                          momentum=0.9,\n",
        "                          weight_decay=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer, milestones=[50, 90, 130], gamma=0.1)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    num_batches = len(train_iter)\n",
        "    for epoch in range(num_epochs):\n",
        "        metric = d2l.Accumulator(3)\n",
        "        net.train()\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            optimizer.zero_grad()\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
        "            train_l = metric[0] / metric[2]\n",
        "            train_acc = metric[1] / metric[2]\n",
        "        scheduler.step()\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
        "        print(f'------- ------- Epoch{epoch+1}/{num_epochs} ------- -------')\n",
        "        print(f'loss {train_l:.5f}, train acc {train_acc:.5f}, '\n",
        "              f'test acc {test_acc:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sbHYLWvQoEC"
      },
      "outputs": [],
      "source": [
        "shape_aug = tv.transforms.RandomResizedCrop(\n",
        "    (28, 28), scale=(0.1, 1), ratio=(0.5, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaEWzquxpRkT"
      },
      "outputs": [],
      "source": [
        "train_augs = tv.transforms.Compose([\n",
        "    tv.transforms.RandomCrop(32, padding=4),\n",
        "    tv.transforms.RandomHorizontalFlip(),\n",
        "    tv.transforms.ToTensor(), shape_aug,\n",
        "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "test_augs = tv.transforms.Compose([\n",
        "    tv.transforms.ToTensor(),\n",
        "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGpyvAuKpOls"
      },
      "outputs": [],
      "source": [
        "def load_cifar10(is_train, augs, batch_size):\n",
        "    dataset = tv.datasets.CIFAR10(root=\"../data\", train=is_train,\n",
        "                                           transform=augs, download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                    shuffle=is_train, num_workers=4)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcP3YUwFVBbn"
      },
      "outputs": [],
      "source": [
        "lr, num_epochs, batch_size = 0.01, 150, 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECkYC1zDVVkL"
      },
      "outputs": [],
      "source": [
        "train_iter = load_cifar10(True, train_augs, batch_size)\n",
        "test_iter = load_cifar10(False, test_augs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIhDRX98UxRx",
        "outputId": "6804af08-da06-4cb8-f10f-70d8fbffa436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n",
            "------- ------- Epoch1/150 ------- -------\n",
            "loss 1.99659, train acc 0.26668, test acc 0.39980\n",
            "------- ------- Epoch2/150 ------- -------\n",
            "loss 1.76780, train acc 0.34798, test acc 0.38470\n",
            "------- ------- Epoch3/150 ------- -------\n",
            "loss 1.66481, train acc 0.39194, test acc 0.39080\n",
            "------- ------- Epoch4/150 ------- -------\n",
            "loss 1.59918, train acc 0.41770, test acc 0.46860\n",
            "------- ------- Epoch5/150 ------- -------\n",
            "loss 1.54562, train acc 0.44266, test acc 0.47070\n",
            "------- ------- Epoch6/150 ------- -------\n",
            "loss 1.49760, train acc 0.46194, test acc 0.49690\n",
            "------- ------- Epoch7/150 ------- -------\n",
            "loss 1.45974, train acc 0.47534, test acc 0.55370\n",
            "------- ------- Epoch8/150 ------- -------\n",
            "loss 1.41487, train acc 0.49520, test acc 0.56930\n",
            "------- ------- Epoch9/150 ------- -------\n",
            "loss 1.38477, train acc 0.50514, test acc 0.54410\n",
            "------- ------- Epoch10/150 ------- -------\n",
            "loss 1.36752, train acc 0.51356, test acc 0.58280\n",
            "------- ------- Epoch11/150 ------- -------\n",
            "loss 1.33326, train acc 0.52664, test acc 0.60410\n",
            "------- ------- Epoch12/150 ------- -------\n",
            "loss 1.30539, train acc 0.53720, test acc 0.63550\n",
            "------- ------- Epoch13/150 ------- -------\n",
            "loss 1.29055, train acc 0.54432, test acc 0.61010\n",
            "------- ------- Epoch14/150 ------- -------\n",
            "loss 1.26513, train acc 0.55672, test acc 0.61910\n",
            "------- ------- Epoch15/150 ------- -------\n",
            "loss 1.23870, train acc 0.56568, test acc 0.64250\n",
            "------- ------- Epoch16/150 ------- -------\n",
            "loss 1.22292, train acc 0.57060, test acc 0.65610\n",
            "------- ------- Epoch17/150 ------- -------\n",
            "loss 1.19503, train acc 0.57900, test acc 0.67990\n",
            "------- ------- Epoch18/150 ------- -------\n",
            "loss 1.18759, train acc 0.58428, test acc 0.69440\n",
            "------- ------- Epoch19/150 ------- -------\n",
            "loss 1.16696, train acc 0.58998, test acc 0.69470\n",
            "------- ------- Epoch20/150 ------- -------\n",
            "loss 1.14991, train acc 0.59814, test acc 0.69490\n",
            "------- ------- Epoch21/150 ------- -------\n",
            "loss 1.13746, train acc 0.60228, test acc 0.70940\n",
            "------- ------- Epoch22/150 ------- -------\n",
            "loss 1.12471, train acc 0.60734, test acc 0.70830\n",
            "------- ------- Epoch23/150 ------- -------\n",
            "loss 1.11289, train acc 0.61316, test acc 0.70090\n",
            "------- ------- Epoch24/150 ------- -------\n",
            "loss 1.11028, train acc 0.61310, test acc 0.72360\n",
            "------- ------- Epoch25/150 ------- -------\n",
            "loss 1.08754, train acc 0.62298, test acc 0.71510\n",
            "------- ------- Epoch26/150 ------- -------\n",
            "loss 1.07817, train acc 0.62346, test acc 0.72260\n",
            "------- ------- Epoch27/150 ------- -------\n",
            "loss 1.07214, train acc 0.62508, test acc 0.71280\n",
            "------- ------- Epoch28/150 ------- -------\n",
            "loss 1.05776, train acc 0.63120, test acc 0.71200\n",
            "------- ------- Epoch29/150 ------- -------\n",
            "loss 1.04758, train acc 0.63482, test acc 0.73280\n",
            "------- ------- Epoch30/150 ------- -------\n",
            "loss 1.04561, train acc 0.63842, test acc 0.71560\n",
            "------- ------- Epoch31/150 ------- -------\n",
            "loss 1.03978, train acc 0.64072, test acc 0.74990\n",
            "------- ------- Epoch32/150 ------- -------\n",
            "loss 1.03849, train acc 0.63864, test acc 0.71500\n",
            "------- ------- Epoch33/150 ------- -------\n",
            "loss 1.02095, train acc 0.64718, test acc 0.72770\n",
            "------- ------- Epoch34/150 ------- -------\n",
            "loss 1.02160, train acc 0.64674, test acc 0.75180\n",
            "------- ------- Epoch35/150 ------- -------\n",
            "loss 1.01674, train acc 0.64864, test acc 0.71190\n",
            "------- ------- Epoch36/150 ------- -------\n",
            "loss 1.01261, train acc 0.64922, test acc 0.75420\n",
            "------- ------- Epoch37/150 ------- -------\n",
            "loss 1.00529, train acc 0.65238, test acc 0.71050\n",
            "------- ------- Epoch38/150 ------- -------\n",
            "loss 1.00386, train acc 0.65074, test acc 0.73990\n",
            "------- ------- Epoch39/150 ------- -------\n",
            "loss 0.98391, train acc 0.65804, test acc 0.77340\n",
            "------- ------- Epoch40/150 ------- -------\n",
            "loss 0.98463, train acc 0.65802, test acc 0.76010\n",
            "------- ------- Epoch41/150 ------- -------\n",
            "loss 0.98760, train acc 0.65816, test acc 0.75890\n",
            "------- ------- Epoch42/150 ------- -------\n",
            "loss 0.99107, train acc 0.65746, test acc 0.71380\n",
            "------- ------- Epoch43/150 ------- -------\n",
            "loss 0.97980, train acc 0.65918, test acc 0.75570\n",
            "------- ------- Epoch44/150 ------- -------\n",
            "loss 0.97792, train acc 0.65966, test acc 0.77240\n",
            "------- ------- Epoch45/150 ------- -------\n",
            "loss 0.97315, train acc 0.66380, test acc 0.77780\n",
            "------- ------- Epoch46/150 ------- -------\n",
            "loss 0.96870, train acc 0.66436, test acc 0.70620\n",
            "------- ------- Epoch47/150 ------- -------\n",
            "loss 0.96243, train acc 0.66388, test acc 0.78770\n",
            "------- ------- Epoch48/150 ------- -------\n",
            "loss 0.95929, train acc 0.66634, test acc 0.77070\n",
            "------- ------- Epoch49/150 ------- -------\n",
            "loss 0.95592, train acc 0.66964, test acc 0.76970\n",
            "------- ------- Epoch50/150 ------- -------\n",
            "loss 0.95376, train acc 0.66846, test acc 0.79390\n",
            "------- ------- Epoch51/150 ------- -------\n",
            "loss 0.84761, train acc 0.71026, test acc 0.83630\n",
            "------- ------- Epoch52/150 ------- -------\n",
            "loss 0.81463, train acc 0.71918, test acc 0.83840\n",
            "------- ------- Epoch53/150 ------- -------\n",
            "loss 0.80248, train acc 0.72376, test acc 0.84300\n",
            "------- ------- Epoch54/150 ------- -------\n",
            "loss 0.79511, train acc 0.72682, test acc 0.84170\n",
            "------- ------- Epoch55/150 ------- -------\n",
            "loss 0.78603, train acc 0.72988, test acc 0.84620\n",
            "------- ------- Epoch56/150 ------- -------\n",
            "loss 0.78381, train acc 0.72912, test acc 0.84850\n",
            "------- ------- Epoch57/150 ------- -------\n",
            "loss 0.77888, train acc 0.73276, test acc 0.84940\n",
            "------- ------- Epoch58/150 ------- -------\n",
            "loss 0.77530, train acc 0.73210, test acc 0.84690\n",
            "------- ------- Epoch59/150 ------- -------\n",
            "loss 0.77330, train acc 0.73384, test acc 0.84890\n",
            "------- ------- Epoch60/150 ------- -------\n",
            "loss 0.77318, train acc 0.73368, test acc 0.85260\n",
            "------- ------- Epoch61/150 ------- -------\n",
            "loss 0.77183, train acc 0.73380, test acc 0.85350\n",
            "------- ------- Epoch62/150 ------- -------\n",
            "loss 0.76773, train acc 0.73542, test acc 0.84780\n",
            "------- ------- Epoch63/150 ------- -------\n",
            "loss 0.77001, train acc 0.73600, test acc 0.85220\n",
            "------- ------- Epoch64/150 ------- -------\n",
            "loss 0.76039, train acc 0.73822, test acc 0.85500\n",
            "------- ------- Epoch65/150 ------- -------\n",
            "loss 0.75780, train acc 0.73938, test acc 0.85330\n",
            "------- ------- Epoch66/150 ------- -------\n",
            "loss 0.75979, train acc 0.73880, test acc 0.84600\n",
            "------- ------- Epoch67/150 ------- -------\n",
            "loss 0.75600, train acc 0.74076, test acc 0.85420\n",
            "------- ------- Epoch68/150 ------- -------\n",
            "loss 0.75356, train acc 0.73814, test acc 0.85730\n",
            "------- ------- Epoch69/150 ------- -------\n",
            "loss 0.75038, train acc 0.74126, test acc 0.85540\n",
            "------- ------- Epoch70/150 ------- -------\n",
            "loss 0.75510, train acc 0.73986, test acc 0.84460\n",
            "------- ------- Epoch71/150 ------- -------\n",
            "loss 0.75612, train acc 0.73744, test acc 0.84760\n",
            "------- ------- Epoch72/150 ------- -------\n",
            "loss 0.74722, train acc 0.74306, test acc 0.85880\n",
            "------- ------- Epoch73/150 ------- -------\n",
            "loss 0.75271, train acc 0.74156, test acc 0.85540\n",
            "------- ------- Epoch74/150 ------- -------\n",
            "loss 0.74439, train acc 0.74296, test acc 0.85830\n",
            "------- ------- Epoch75/150 ------- -------\n",
            "loss 0.74065, train acc 0.74548, test acc 0.85960\n",
            "------- ------- Epoch76/150 ------- -------\n",
            "loss 0.73945, train acc 0.74644, test acc 0.85470\n",
            "------- ------- Epoch77/150 ------- -------\n",
            "loss 0.74784, train acc 0.74292, test acc 0.85630\n",
            "------- ------- Epoch78/150 ------- -------\n",
            "loss 0.74547, train acc 0.74270, test acc 0.86070\n",
            "------- ------- Epoch79/150 ------- -------\n",
            "loss 0.74045, train acc 0.74516, test acc 0.86150\n",
            "------- ------- Epoch80/150 ------- -------\n",
            "loss 0.73541, train acc 0.74436, test acc 0.85190\n",
            "------- ------- Epoch81/150 ------- -------\n",
            "loss 0.73933, train acc 0.74492, test acc 0.86680\n",
            "------- ------- Epoch82/150 ------- -------\n",
            "loss 0.73376, train acc 0.74724, test acc 0.86140\n",
            "------- ------- Epoch83/150 ------- -------\n",
            "loss 0.73699, train acc 0.74784, test acc 0.85460\n",
            "------- ------- Epoch84/150 ------- -------\n",
            "loss 0.73115, train acc 0.74776, test acc 0.86090\n",
            "------- ------- Epoch85/150 ------- -------\n",
            "loss 0.73026, train acc 0.75034, test acc 0.85480\n",
            "------- ------- Epoch86/150 ------- -------\n",
            "loss 0.73059, train acc 0.74936, test acc 0.85870\n",
            "------- ------- Epoch87/150 ------- -------\n",
            "loss 0.73041, train acc 0.74976, test acc 0.86120\n",
            "------- ------- Epoch88/150 ------- -------\n",
            "loss 0.73501, train acc 0.74458, test acc 0.85030\n",
            "------- ------- Epoch89/150 ------- -------\n",
            "loss 0.73112, train acc 0.74994, test acc 0.85670\n",
            "------- ------- Epoch90/150 ------- -------\n",
            "loss 0.73161, train acc 0.74702, test acc 0.86010\n",
            "------- ------- Epoch91/150 ------- -------\n",
            "loss 0.71343, train acc 0.75412, test acc 0.86520\n",
            "------- ------- Epoch92/150 ------- -------\n",
            "loss 0.70661, train acc 0.75778, test acc 0.86680\n",
            "------- ------- Epoch93/150 ------- -------\n",
            "loss 0.69667, train acc 0.76308, test acc 0.87110\n",
            "------- ------- Epoch94/150 ------- -------\n",
            "loss 0.69772, train acc 0.76180, test acc 0.86930\n",
            "------- ------- Epoch95/150 ------- -------\n",
            "loss 0.69797, train acc 0.76306, test acc 0.87060\n",
            "------- ------- Epoch96/150 ------- -------\n",
            "loss 0.70606, train acc 0.75892, test acc 0.87070\n",
            "------- ------- Epoch97/150 ------- -------\n",
            "loss 0.69225, train acc 0.75972, test acc 0.87150\n",
            "------- ------- Epoch98/150 ------- -------\n",
            "loss 0.68992, train acc 0.76544, test acc 0.87030\n",
            "------- ------- Epoch99/150 ------- -------\n",
            "loss 0.69983, train acc 0.76246, test acc 0.87140\n",
            "------- ------- Epoch100/150 ------- -------\n",
            "loss 0.69886, train acc 0.75920, test acc 0.86960\n",
            "------- ------- Epoch101/150 ------- -------\n",
            "loss 0.69390, train acc 0.76134, test acc 0.87060\n",
            "------- ------- Epoch102/150 ------- -------\n",
            "loss 0.69243, train acc 0.76434, test acc 0.87140\n",
            "------- ------- Epoch103/150 ------- -------\n",
            "loss 0.69474, train acc 0.75974, test acc 0.87110\n",
            "------- ------- Epoch104/150 ------- -------\n",
            "loss 0.69587, train acc 0.76224, test acc 0.87260\n",
            "------- ------- Epoch105/150 ------- -------\n",
            "loss 0.69280, train acc 0.76358, test acc 0.87180\n",
            "------- ------- Epoch106/150 ------- -------\n",
            "loss 0.69509, train acc 0.76228, test acc 0.87110\n",
            "------- ------- Epoch107/150 ------- -------\n",
            "loss 0.68804, train acc 0.76288, test acc 0.87060\n",
            "------- ------- Epoch108/150 ------- -------\n",
            "loss 0.68508, train acc 0.76320, test acc 0.87140\n",
            "------- ------- Epoch109/150 ------- -------\n",
            "loss 0.69035, train acc 0.76282, test acc 0.87130\n",
            "------- ------- Epoch110/150 ------- -------\n",
            "loss 0.69001, train acc 0.76254, test acc 0.87030\n",
            "------- ------- Epoch111/150 ------- -------\n",
            "loss 0.69550, train acc 0.76110, test acc 0.87420\n",
            "------- ------- Epoch112/150 ------- -------\n",
            "loss 0.68791, train acc 0.76176, test acc 0.87060\n",
            "------- ------- Epoch113/150 ------- -------\n",
            "loss 0.68839, train acc 0.76186, test acc 0.87150\n",
            "------- ------- Epoch114/150 ------- -------\n",
            "loss 0.68910, train acc 0.76178, test acc 0.87180\n",
            "------- ------- Epoch115/150 ------- -------\n",
            "loss 0.68478, train acc 0.76650, test acc 0.87190\n",
            "------- ------- Epoch116/150 ------- -------\n",
            "loss 0.68672, train acc 0.76376, test acc 0.87420\n",
            "------- ------- Epoch117/150 ------- -------\n",
            "loss 0.68794, train acc 0.76472, test acc 0.87270\n",
            "------- ------- Epoch118/150 ------- -------\n",
            "loss 0.68747, train acc 0.76434, test acc 0.87300\n",
            "------- ------- Epoch119/150 ------- -------\n",
            "loss 0.68054, train acc 0.76484, test acc 0.87290\n",
            "------- ------- Epoch120/150 ------- -------\n",
            "loss 0.68314, train acc 0.76588, test acc 0.87180\n",
            "------- ------- Epoch121/150 ------- -------\n",
            "loss 0.68216, train acc 0.76788, test acc 0.87180\n",
            "------- ------- Epoch122/150 ------- -------\n",
            "loss 0.68645, train acc 0.76490, test acc 0.87280\n",
            "------- ------- Epoch123/150 ------- -------\n",
            "loss 0.68437, train acc 0.76578, test acc 0.87260\n",
            "------- ------- Epoch124/150 ------- -------\n",
            "loss 0.68738, train acc 0.76652, test acc 0.87350\n",
            "------- ------- Epoch125/150 ------- -------\n",
            "loss 0.67837, train acc 0.76730, test acc 0.87180\n",
            "------- ------- Epoch126/150 ------- -------\n",
            "loss 0.68451, train acc 0.76522, test acc 0.87280\n",
            "------- ------- Epoch127/150 ------- -------\n",
            "loss 0.68414, train acc 0.76348, test acc 0.87350\n",
            "------- ------- Epoch128/150 ------- -------\n",
            "loss 0.68363, train acc 0.76580, test acc 0.87290\n",
            "------- ------- Epoch129/150 ------- -------\n",
            "loss 0.68711, train acc 0.76308, test acc 0.87390\n",
            "------- ------- Epoch130/150 ------- -------\n",
            "loss 0.68751, train acc 0.76534, test acc 0.87150\n",
            "------- ------- Epoch131/150 ------- -------\n",
            "loss 0.68118, train acc 0.76646, test acc 0.87360\n",
            "------- ------- Epoch132/150 ------- -------\n",
            "loss 0.67590, train acc 0.76818, test acc 0.87430\n",
            "------- ------- Epoch133/150 ------- -------\n",
            "loss 0.67878, train acc 0.76866, test acc 0.87320\n",
            "------- ------- Epoch134/150 ------- -------\n",
            "loss 0.68171, train acc 0.76566, test acc 0.87280\n",
            "------- ------- Epoch135/150 ------- -------\n",
            "loss 0.68430, train acc 0.76560, test acc 0.87240\n",
            "------- ------- Epoch136/150 ------- -------\n",
            "loss 0.67452, train acc 0.76800, test acc 0.87200\n",
            "------- ------- Epoch137/150 ------- -------\n",
            "loss 0.67921, train acc 0.76704, test acc 0.87250\n",
            "------- ------- Epoch138/150 ------- -------\n",
            "loss 0.67777, train acc 0.76886, test acc 0.87270\n",
            "------- ------- Epoch139/150 ------- -------\n",
            "loss 0.68058, train acc 0.76790, test acc 0.87310\n",
            "------- ------- Epoch140/150 ------- -------\n",
            "loss 0.68187, train acc 0.76372, test acc 0.87230\n",
            "------- ------- Epoch141/150 ------- -------\n",
            "loss 0.68305, train acc 0.76658, test acc 0.87150\n",
            "------- ------- Epoch142/150 ------- -------\n",
            "loss 0.68120, train acc 0.76576, test acc 0.87310\n",
            "------- ------- Epoch143/150 ------- -------\n",
            "loss 0.68446, train acc 0.76560, test acc 0.87250\n",
            "------- ------- Epoch144/150 ------- -------\n",
            "loss 0.67805, train acc 0.76624, test acc 0.87320\n",
            "------- ------- Epoch145/150 ------- -------\n",
            "loss 0.68398, train acc 0.76566, test acc 0.87170\n",
            "------- ------- Epoch146/150 ------- -------\n",
            "loss 0.68538, train acc 0.76560, test acc 0.87150\n",
            "------- ------- Epoch147/150 ------- -------\n",
            "loss 0.67944, train acc 0.76560, test acc 0.87370\n",
            "------- ------- Epoch148/150 ------- -------\n",
            "loss 0.67398, train acc 0.76908, test acc 0.87130\n",
            "------- ------- Epoch149/150 ------- -------\n",
            "loss 0.68162, train acc 0.76690, test acc 0.87150\n",
            "------- ------- Epoch150/150 ------- -------\n",
            "loss 0.68300, train acc 0.76666, test acc 0.87350\n"
          ]
        }
      ],
      "source": [
        "train_net(net, train_iter, test_iter, num_epochs, lr, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ResNet_20.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
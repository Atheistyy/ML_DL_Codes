{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k7N4APtxyZLd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "import torchvision as tv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0SQvlqb869Sx"
   },
   "outputs": [],
   "source": [
    "class SE_Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1, ratio=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "        # SE layers\n",
    "        self.fc1 = nn.Conv2d(num_channels, num_channels//ratio, kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(num_channels//ratio, num_channels, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        \n",
    "        # Squeeze\n",
    "        z = F.avg_pool2d(Y, Y.size(2))\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.sigmoid(self.fc2(z))\n",
    "\n",
    "        # Excitation\n",
    "        Y = Y * z \n",
    "\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1UOA_OEcFq9d"
   },
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=1, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x7vIM9dY69Zz"
   },
   "outputs": [],
   "source": [
    "def se_resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(SE_Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(SE_Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9YmVRD1_69cY"
   },
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*se_resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*se_resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*se_resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*se_resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OAUoGUrLHUDh"
   },
   "outputs": [],
   "source": [
    "se_net18 = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(), nn.Dropout(0.25), nn.Linear(512, 10))\n",
    "\n",
    "if device == 'cuda':\n",
    "\tse_net18 = torch.nn.DataParallel(se_net18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xzf_6-48H0h6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 32, 32])\n",
      "Sequential output shape:\t torch.Size([1, 64, 32, 32])\n",
      "Sequential output shape:\t torch.Size([1, 128, 16, 16])\n",
      "Sequential output shape:\t torch.Size([1, 256, 8, 8])\n",
      "Sequential output shape:\t torch.Size([1, 512, 4, 4])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Dropout output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 3, 32, 32))\n",
    "for layer in se_net18:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XAT7w_MlUbbK"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None): \n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  \n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OaicZn8ZPJHn"
   },
   "outputs": [],
   "source": [
    "def train_net(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            torch.nn.init.kaiming_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(params=net.parameters(),\n",
    "                          lr=lr,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[50, 90, 120], gamma=0.1)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    num_batches = len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "        scheduler.step()\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(f'------- ------- Epoch{epoch+1}/{num_epochs} ------- -------')\n",
    "        print(f'loss {train_l:.5f}, train acc {train_acc:.5f}, '\n",
    "              f'test acc {test_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xO4oVdH41Dfz"
   },
   "outputs": [],
   "source": [
    "shape_aug = tv.transforms.RandomResizedCrop(\n",
    "    (28, 28), scale=(0.1, 1), ratio=(0.5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YaEWzquxpRkT"
   },
   "outputs": [],
   "source": [
    "train_augs = tv.transforms.Compose([\n",
    "    tv.transforms.RandomCrop(32, padding=4),\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.ToTensor(), shape_aug,\n",
    "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_augs = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qGpyvAuKpOls"
   },
   "outputs": [],
   "source": [
    "def load_cifar10(is_train, augs, batch_size):\n",
    "    dataset = tv.datasets.CIFAR10(root=\"../data\", train=is_train,\n",
    "                                           transform=augs, download=True)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                    shuffle=is_train, num_workers=4)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RcP3YUwFVBbn"
   },
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.01, 150, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ECkYC1zDVVkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_iter = load_cifar10(True, train_augs, batch_size)\n",
    "test_iter = load_cifar10(False, test_augs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cIhDRX98UxRx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "------- ------- Epoch1/150 ------- -------\n",
      "loss 1.85227, train acc 0.32598, test acc 0.41650\n",
      "------- ------- Epoch2/150 ------- -------\n",
      "loss 1.57129, train acc 0.42900, test acc 0.52000\n",
      "------- ------- Epoch3/150 ------- -------\n",
      "loss 1.44139, train acc 0.48232, test acc 0.59330\n",
      "------- ------- Epoch4/150 ------- -------\n",
      "loss 1.35933, train acc 0.51432, test acc 0.59530\n",
      "------- ------- Epoch5/150 ------- -------\n",
      "loss 1.27316, train acc 0.54420, test acc 0.64990\n",
      "------- ------- Epoch6/150 ------- -------\n",
      "loss 1.21804, train acc 0.56672, test acc 0.61900\n",
      "------- ------- Epoch7/150 ------- -------\n",
      "loss 1.16100, train acc 0.58604, test acc 0.70520\n",
      "------- ------- Epoch8/150 ------- -------\n",
      "loss 1.12381, train acc 0.59980, test acc 0.65930\n",
      "------- ------- Epoch9/150 ------- -------\n",
      "loss 1.09051, train acc 0.61760, test acc 0.71820\n",
      "------- ------- Epoch10/150 ------- -------\n",
      "loss 1.05448, train acc 0.62992, test acc 0.76170\n",
      "------- ------- Epoch11/150 ------- -------\n",
      "loss 1.02768, train acc 0.64028, test acc 0.74320\n",
      "------- ------- Epoch12/150 ------- -------\n",
      "loss 0.99377, train acc 0.64912, test acc 0.76900\n",
      "------- ------- Epoch13/150 ------- -------\n",
      "loss 0.97372, train acc 0.65948, test acc 0.76400\n",
      "------- ------- Epoch14/150 ------- -------\n",
      "loss 0.96228, train acc 0.66346, test acc 0.75640\n",
      "------- ------- Epoch15/150 ------- -------\n",
      "loss 0.93768, train acc 0.67284, test acc 0.74580\n",
      "------- ------- Epoch16/150 ------- -------\n",
      "loss 0.91638, train acc 0.68084, test acc 0.77060\n",
      "------- ------- Epoch17/150 ------- -------\n",
      "loss 0.90841, train acc 0.68500, test acc 0.77850\n",
      "------- ------- Epoch18/150 ------- -------\n",
      "loss 0.89681, train acc 0.68940, test acc 0.79730\n",
      "------- ------- Epoch19/150 ------- -------\n",
      "loss 0.87858, train acc 0.69348, test acc 0.75330\n",
      "------- ------- Epoch20/150 ------- -------\n",
      "loss 0.86141, train acc 0.70114, test acc 0.78690\n",
      "------- ------- Epoch21/150 ------- -------\n",
      "loss 0.85907, train acc 0.70222, test acc 0.80240\n",
      "------- ------- Epoch22/150 ------- -------\n",
      "loss 0.84327, train acc 0.70526, test acc 0.79150\n",
      "------- ------- Epoch23/150 ------- -------\n",
      "loss 0.82939, train acc 0.71234, test acc 0.79590\n",
      "------- ------- Epoch24/150 ------- -------\n",
      "loss 0.81934, train acc 0.71638, test acc 0.79060\n",
      "------- ------- Epoch25/150 ------- -------\n",
      "loss 0.81990, train acc 0.71798, test acc 0.84060\n",
      "------- ------- Epoch26/150 ------- -------\n",
      "loss 0.80042, train acc 0.72268, test acc 0.82630\n",
      "------- ------- Epoch27/150 ------- -------\n",
      "loss 0.79537, train acc 0.72376, test acc 0.81710\n",
      "------- ------- Epoch28/150 ------- -------\n",
      "loss 0.79998, train acc 0.72302, test acc 0.83190\n",
      "------- ------- Epoch29/150 ------- -------\n",
      "loss 0.78965, train acc 0.72688, test acc 0.82590\n",
      "------- ------- Epoch30/150 ------- -------\n",
      "loss 0.78883, train acc 0.72596, test acc 0.84440\n",
      "------- ------- Epoch31/150 ------- -------\n",
      "loss 0.76957, train acc 0.73494, test acc 0.82210\n",
      "------- ------- Epoch32/150 ------- -------\n",
      "loss 0.76960, train acc 0.73416, test acc 0.83520\n",
      "------- ------- Epoch33/150 ------- -------\n",
      "loss 0.76126, train acc 0.73748, test acc 0.82160\n",
      "------- ------- Epoch34/150 ------- -------\n",
      "loss 0.76118, train acc 0.73750, test acc 0.85440\n",
      "------- ------- Epoch35/150 ------- -------\n",
      "loss 0.75384, train acc 0.74120, test acc 0.81720\n",
      "------- ------- Epoch36/150 ------- -------\n",
      "loss 0.74627, train acc 0.74150, test acc 0.84630\n",
      "------- ------- Epoch37/150 ------- -------\n",
      "loss 0.74546, train acc 0.74350, test acc 0.82960\n",
      "------- ------- Epoch38/150 ------- -------\n",
      "loss 0.74089, train acc 0.74340, test acc 0.84300\n",
      "------- ------- Epoch39/150 ------- -------\n",
      "loss 0.73640, train acc 0.74814, test acc 0.82790\n",
      "------- ------- Epoch40/150 ------- -------\n",
      "loss 0.73316, train acc 0.74742, test acc 0.85450\n",
      "------- ------- Epoch41/150 ------- -------\n",
      "loss 0.72583, train acc 0.75016, test acc 0.85580\n",
      "------- ------- Epoch42/150 ------- -------\n",
      "loss 0.72161, train acc 0.75078, test acc 0.84180\n",
      "------- ------- Epoch43/150 ------- -------\n",
      "loss 0.72151, train acc 0.75122, test acc 0.85650\n",
      "------- ------- Epoch44/150 ------- -------\n",
      "loss 0.71973, train acc 0.75298, test acc 0.85100\n",
      "------- ------- Epoch45/150 ------- -------\n",
      "loss 0.71474, train acc 0.75340, test acc 0.85080\n",
      "------- ------- Epoch46/150 ------- -------\n",
      "loss 0.70958, train acc 0.75456, test acc 0.79040\n",
      "------- ------- Epoch47/150 ------- -------\n",
      "loss 0.71911, train acc 0.75222, test acc 0.83620\n",
      "------- ------- Epoch48/150 ------- -------\n",
      "loss 0.71131, train acc 0.75448, test acc 0.86060\n",
      "------- ------- Epoch49/150 ------- -------\n",
      "loss 0.70113, train acc 0.75780, test acc 0.85920\n",
      "------- ------- Epoch50/150 ------- -------\n",
      "loss 0.69489, train acc 0.75996, test acc 0.86720\n",
      "------- ------- Epoch51/150 ------- -------\n",
      "loss 0.58677, train acc 0.79996, test acc 0.90270\n",
      "------- ------- Epoch52/150 ------- -------\n",
      "loss 0.53748, train acc 0.81690, test acc 0.90660\n",
      "------- ------- Epoch53/150 ------- -------\n",
      "loss 0.53420, train acc 0.81932, test acc 0.91050\n",
      "------- ------- Epoch54/150 ------- -------\n",
      "loss 0.51801, train acc 0.82512, test acc 0.91080\n",
      "------- ------- Epoch55/150 ------- -------\n",
      "loss 0.50815, train acc 0.82694, test acc 0.91200\n",
      "------- ------- Epoch56/150 ------- -------\n",
      "loss 0.50210, train acc 0.82912, test acc 0.90940\n",
      "------- ------- Epoch57/150 ------- -------\n",
      "loss 0.50169, train acc 0.82970, test acc 0.91280\n",
      "------- ------- Epoch58/150 ------- -------\n",
      "loss 0.48653, train acc 0.83532, test acc 0.91450\n",
      "------- ------- Epoch59/150 ------- -------\n",
      "loss 0.48757, train acc 0.83312, test acc 0.91480\n",
      "------- ------- Epoch60/150 ------- -------\n",
      "loss 0.47940, train acc 0.83578, test acc 0.91600\n",
      "------- ------- Epoch61/150 ------- -------\n",
      "loss 0.48310, train acc 0.83464, test acc 0.91560\n",
      "------- ------- Epoch62/150 ------- -------\n",
      "loss 0.47688, train acc 0.83812, test acc 0.91590\n",
      "------- ------- Epoch63/150 ------- -------\n",
      "loss 0.47572, train acc 0.83738, test acc 0.91750\n",
      "------- ------- Epoch64/150 ------- -------\n",
      "loss 0.46565, train acc 0.83960, test acc 0.91870\n",
      "------- ------- Epoch65/150 ------- -------\n",
      "loss 0.46991, train acc 0.84022, test acc 0.91890\n",
      "------- ------- Epoch66/150 ------- -------\n",
      "loss 0.46405, train acc 0.84180, test acc 0.91810\n",
      "------- ------- Epoch67/150 ------- -------\n",
      "loss 0.46778, train acc 0.84080, test acc 0.91740\n",
      "------- ------- Epoch68/150 ------- -------\n",
      "loss 0.46478, train acc 0.83964, test acc 0.92160\n",
      "------- ------- Epoch69/150 ------- -------\n",
      "loss 0.45892, train acc 0.84340, test acc 0.91770\n",
      "------- ------- Epoch70/150 ------- -------\n",
      "loss 0.45571, train acc 0.84528, test acc 0.91940\n",
      "------- ------- Epoch71/150 ------- -------\n",
      "loss 0.44912, train acc 0.84708, test acc 0.91790\n",
      "------- ------- Epoch72/150 ------- -------\n",
      "loss 0.45329, train acc 0.84616, test acc 0.91460\n",
      "------- ------- Epoch73/150 ------- -------\n",
      "loss 0.45215, train acc 0.84552, test acc 0.92030\n",
      "------- ------- Epoch74/150 ------- -------\n",
      "loss 0.44787, train acc 0.84678, test acc 0.92200\n",
      "------- ------- Epoch75/150 ------- -------\n",
      "loss 0.44436, train acc 0.84928, test acc 0.91420\n",
      "------- ------- Epoch76/150 ------- -------\n",
      "loss 0.43365, train acc 0.85226, test acc 0.91890\n",
      "------- ------- Epoch77/150 ------- -------\n",
      "loss 0.44031, train acc 0.84936, test acc 0.91800\n",
      "------- ------- Epoch78/150 ------- -------\n",
      "loss 0.43447, train acc 0.85074, test acc 0.92180\n",
      "------- ------- Epoch79/150 ------- -------\n",
      "loss 0.43944, train acc 0.84896, test acc 0.91680\n",
      "------- ------- Epoch80/150 ------- -------\n",
      "loss 0.43620, train acc 0.84924, test acc 0.92170\n",
      "------- ------- Epoch81/150 ------- -------\n",
      "loss 0.43012, train acc 0.85268, test acc 0.92020\n",
      "------- ------- Epoch82/150 ------- -------\n",
      "loss 0.42933, train acc 0.85402, test acc 0.92060\n",
      "------- ------- Epoch83/150 ------- -------\n",
      "loss 0.42901, train acc 0.85452, test acc 0.92200\n",
      "------- ------- Epoch84/150 ------- -------\n",
      "loss 0.43122, train acc 0.85104, test acc 0.92140\n",
      "------- ------- Epoch85/150 ------- -------\n",
      "loss 0.42550, train acc 0.85470, test acc 0.92150\n",
      "------- ------- Epoch86/150 ------- -------\n",
      "loss 0.42849, train acc 0.85382, test acc 0.91970\n",
      "------- ------- Epoch87/150 ------- -------\n",
      "loss 0.43013, train acc 0.85126, test acc 0.92110\n",
      "------- ------- Epoch88/150 ------- -------\n",
      "loss 0.41888, train acc 0.85538, test acc 0.92580\n",
      "------- ------- Epoch89/150 ------- -------\n",
      "loss 0.41689, train acc 0.85750, test acc 0.92220\n",
      "------- ------- Epoch90/150 ------- -------\n",
      "loss 0.42599, train acc 0.85340, test acc 0.92190\n",
      "------- ------- Epoch91/150 ------- -------\n",
      "loss 0.40019, train acc 0.86268, test acc 0.92430\n",
      "------- ------- Epoch92/150 ------- -------\n",
      "loss 0.39078, train acc 0.86964, test acc 0.92470\n",
      "------- ------- Epoch93/150 ------- -------\n",
      "loss 0.38756, train acc 0.86834, test acc 0.92660\n",
      "------- ------- Epoch94/150 ------- -------\n",
      "loss 0.38388, train acc 0.86942, test acc 0.92780\n",
      "------- ------- Epoch95/150 ------- -------\n",
      "loss 0.38845, train acc 0.86860, test acc 0.92680\n",
      "------- ------- Epoch96/150 ------- -------\n",
      "loss 0.37849, train acc 0.87102, test acc 0.92740\n",
      "------- ------- Epoch97/150 ------- -------\n",
      "loss 0.37936, train acc 0.87004, test acc 0.92810\n",
      "------- ------- Epoch98/150 ------- -------\n",
      "loss 0.37616, train acc 0.87274, test acc 0.92890\n",
      "------- ------- Epoch99/150 ------- -------\n",
      "loss 0.38177, train acc 0.86802, test acc 0.92900\n",
      "------- ------- Epoch100/150 ------- -------\n",
      "loss 0.38296, train acc 0.86958, test acc 0.92980\n",
      "------- ------- Epoch101/150 ------- -------\n",
      "loss 0.38083, train acc 0.87056, test acc 0.92970\n",
      "------- ------- Epoch102/150 ------- -------\n",
      "loss 0.38118, train acc 0.87024, test acc 0.92860\n",
      "------- ------- Epoch103/150 ------- -------\n",
      "loss 0.38177, train acc 0.87006, test acc 0.92940\n",
      "------- ------- Epoch104/150 ------- -------\n",
      "loss 0.37396, train acc 0.87334, test acc 0.93020\n",
      "------- ------- Epoch105/150 ------- -------\n",
      "loss 0.37075, train acc 0.87430, test acc 0.92980\n",
      "------- ------- Epoch106/150 ------- -------\n",
      "loss 0.37321, train acc 0.87352, test acc 0.93030\n",
      "------- ------- Epoch107/150 ------- -------\n",
      "loss 0.36802, train acc 0.87392, test acc 0.92910\n",
      "------- ------- Epoch108/150 ------- -------\n",
      "loss 0.37677, train acc 0.87052, test acc 0.93030\n",
      "------- ------- Epoch109/150 ------- -------\n",
      "loss 0.36843, train acc 0.87516, test acc 0.92940\n",
      "------- ------- Epoch110/150 ------- -------\n",
      "loss 0.37310, train acc 0.87190, test acc 0.92880\n",
      "------- ------- Epoch111/150 ------- -------\n",
      "loss 0.37282, train acc 0.87270, test acc 0.92920\n",
      "------- ------- Epoch112/150 ------- -------\n",
      "loss 0.36485, train acc 0.87726, test acc 0.92990\n",
      "------- ------- Epoch113/150 ------- -------\n",
      "loss 0.36523, train acc 0.87722, test acc 0.93010\n",
      "------- ------- Epoch114/150 ------- -------\n",
      "loss 0.36937, train acc 0.87530, test acc 0.92850\n",
      "------- ------- Epoch115/150 ------- -------\n",
      "loss 0.37178, train acc 0.87210, test acc 0.92940\n",
      "------- ------- Epoch116/150 ------- -------\n",
      "loss 0.36846, train acc 0.87628, test acc 0.92910\n",
      "------- ------- Epoch117/150 ------- -------\n",
      "loss 0.37082, train acc 0.87374, test acc 0.92980\n",
      "------- ------- Epoch118/150 ------- -------\n",
      "loss 0.36866, train acc 0.87404, test acc 0.92990\n",
      "------- ------- Epoch119/150 ------- -------\n",
      "loss 0.36757, train acc 0.87464, test acc 0.92920\n",
      "------- ------- Epoch120/150 ------- -------\n",
      "loss 0.36553, train acc 0.87580, test acc 0.92900\n",
      "------- ------- Epoch121/150 ------- -------\n",
      "loss 0.35852, train acc 0.87784, test acc 0.93070\n",
      "------- ------- Epoch122/150 ------- -------\n",
      "loss 0.36381, train acc 0.87502, test acc 0.92950\n",
      "------- ------- Epoch123/150 ------- -------\n",
      "loss 0.36487, train acc 0.87446, test acc 0.92960\n",
      "------- ------- Epoch124/150 ------- -------\n",
      "loss 0.36564, train acc 0.87646, test acc 0.92910\n",
      "------- ------- Epoch125/150 ------- -------\n",
      "loss 0.36497, train acc 0.87622, test acc 0.92880\n",
      "------- ------- Epoch126/150 ------- -------\n",
      "loss 0.36467, train acc 0.87616, test acc 0.93020\n",
      "------- ------- Epoch127/150 ------- -------\n",
      "loss 0.36395, train acc 0.87542, test acc 0.93000\n",
      "------- ------- Epoch128/150 ------- -------\n",
      "loss 0.36712, train acc 0.87452, test acc 0.93050\n",
      "------- ------- Epoch129/150 ------- -------\n",
      "loss 0.36666, train acc 0.87474, test acc 0.92930\n",
      "------- ------- Epoch130/150 ------- -------\n",
      "loss 0.36516, train acc 0.87576, test acc 0.93140\n",
      "------- ------- Epoch131/150 ------- -------\n",
      "loss 0.36588, train acc 0.87688, test acc 0.92990\n",
      "------- ------- Epoch132/150 ------- -------\n",
      "loss 0.35945, train acc 0.87700, test acc 0.93080\n",
      "------- ------- Epoch133/150 ------- -------\n",
      "loss 0.36833, train acc 0.87510, test acc 0.93000\n",
      "------- ------- Epoch134/150 ------- -------\n",
      "loss 0.36761, train acc 0.87454, test acc 0.93030\n",
      "------- ------- Epoch135/150 ------- -------\n",
      "loss 0.36007, train acc 0.87758, test acc 0.93090\n",
      "------- ------- Epoch136/150 ------- -------\n",
      "loss 0.36763, train acc 0.87554, test acc 0.92970\n",
      "------- ------- Epoch137/150 ------- -------\n",
      "loss 0.36560, train acc 0.87522, test acc 0.93040\n",
      "------- ------- Epoch138/150 ------- -------\n",
      "loss 0.36336, train acc 0.87644, test acc 0.93000\n",
      "------- ------- Epoch139/150 ------- -------\n",
      "loss 0.36109, train acc 0.87632, test acc 0.93000\n",
      "------- ------- Epoch140/150 ------- -------\n",
      "loss 0.36655, train acc 0.87438, test acc 0.93080\n",
      "------- ------- Epoch141/150 ------- -------\n",
      "loss 0.36149, train acc 0.87628, test acc 0.93040\n",
      "------- ------- Epoch142/150 ------- -------\n",
      "loss 0.36348, train acc 0.87532, test acc 0.93090\n",
      "------- ------- Epoch143/150 ------- -------\n",
      "loss 0.36590, train acc 0.87626, test acc 0.92900\n",
      "------- ------- Epoch144/150 ------- -------\n",
      "loss 0.36061, train acc 0.87708, test acc 0.93010\n",
      "------- ------- Epoch145/150 ------- -------\n",
      "loss 0.36084, train acc 0.87632, test acc 0.93020\n",
      "------- ------- Epoch146/150 ------- -------\n",
      "loss 0.36451, train acc 0.87536, test acc 0.93070\n",
      "------- ------- Epoch147/150 ------- -------\n",
      "loss 0.36282, train acc 0.87780, test acc 0.93030\n",
      "------- ------- Epoch148/150 ------- -------\n",
      "loss 0.36282, train acc 0.87710, test acc 0.93040\n",
      "------- ------- Epoch149/150 ------- -------\n",
      "loss 0.36298, train acc 0.87648, test acc 0.93000\n",
      "------- ------- Epoch150/150 ------- -------\n",
      "loss 0.36599, train acc 0.87506, test acc 0.92950\n"
     ]
    }
   ],
   "source": [
    "train_net(se_net18, train_iter, test_iter, num_epochs, lr, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SE_ResNet_18.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
